{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### * While the original source code is written in Pytorch, the below code is adapted to Tensorflow.\n",
        "\n",
        "- GPU utilization not enabled"
      ],
      "metadata": {
        "id": "xmBdax23tCen"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7ep-bhusdrP"
      },
      "source": [
        "# 1. Preparing the tinyshakespeare text file for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "73HKWQbFsdrO",
        "outputId": "34a30494-0117-41ad-fbbd-1e406f800916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1089k  100 1089k    0     0  1749k      0 --:--:-- --:--:-- --:--:-- 1748k\n"
          ]
        }
      ],
      "source": [
        "# Downloading tinyshakesphere for training\n",
        "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t-iatUv1sdrQ",
        "outputId": "e5589b95-a8ec-410e-8568-aae11607bc6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1115394 characters in the dataset\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the text file\n",
        "with open('tinyshakespeare.txt','r') as file:\n",
        "    text = file.read()\n",
        "print(f'There are {len(text)} characters in the dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FpDFL78AsdrR",
        "outputId": "8248245e-4a32-437c-ffa4-2000db4e1d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# Printing the first 1000 characters\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aOQjexVqsdrR",
        "outputId": "40af24c8-49a4-4827-cc83-8474e8d485b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique characters (including white space): 65\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "# Identifying the number of unique characters contained in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Number of unique characters (including white space): {vocab_size}{''.join(chars)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55wnbME6sdrR"
      },
      "source": [
        "# 2. Basic mapping between characters to integers\n",
        "\n",
        "Tokenizing at the character-level.\n",
        "\n",
        "More sophisticated examples of word encoding include Google's SentencePiece and OpenAI's tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tq5eWRmEsdrR",
        "outputId": "9c3f50cf-39b1-4dc7-92aa-c1b14b98d46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 52, 1, 42, 47, 45, 47, 58, 57]\n",
            "Shakespeare in digits\n"
          ]
        }
      ],
      "source": [
        "# Assigning numbers to each characters to encode the characters to integers\n",
        "ctoi = {char : num for num, char in enumerate(chars)}\n",
        "encode = lambda s: [ctoi[c] for c in s]\n",
        "print(encode('Shakespeare in digits'))\n",
        "\n",
        "# Reversely, decode integers back to characters\n",
        "itoc = {num : char for num, char in enumerate(chars)}\n",
        "decode = lambda l : ''.join([itoc[i] for i in l])\n",
        "print(decode(encode('Shakespeare in digits')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I6HH69LpsdrR",
        "outputId": "0ebe1d05-56f7-4e6a-9219-cbab913e5836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1115394,) <dtype: 'int32'>\n",
            "tf.Tensor(\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59], shape=(100,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Tokenizing the total text\n",
        "import tensorflow as tf\n",
        "data = tf.convert_to_tensor(encode(text))\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4XHVnVZXsdrR",
        "outputId": "10eee96f-f43d-4a51-ad77-3bba9643db65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train data : 1003854\n",
            "Length of test data : 111540\n"
          ]
        }
      ],
      "source": [
        "# Train and validation split sets, with 9:1 ratio\n",
        "n = int(0.9*len(data))\n",
        "data_train = data[:n]\n",
        "data_test = data[n:]\n",
        "print(f'Length of train data : {len(data_train)}\\nLength of test data : {len(data_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nT2C2TlIsdrS",
        "outputId": "9eb40612-cdbe-4c4c-ace2-d4693c8253eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int32)\n",
            "Input : [18], Output : 47\n",
            "Input : [18 47], Output : 56\n",
            "Input : [18 47 56], Output : 57\n",
            "Input : [18 47 56 57], Output : 58\n",
            "Input : [18 47 56 57 58], Output : 1\n",
            "Input : [18 47 56 57 58  1], Output : 15\n",
            "Input : [18 47 56 57 58  1 15], Output : 47\n",
            "Input : [18 47 56 57 58  1 15 47], Output : 58\n"
          ]
        }
      ],
      "source": [
        "# Starting with block_size implementation\n",
        "block_size = 8                            # Context length\n",
        "print(data_train[:block_size + 1])\n",
        "x = data_train[:block_size]               # Initial block-size\n",
        "y = data_train[1:block_size+1]            # Next block-size\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'Input : {context}, Output : {target}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fpT9xJShsdrS",
        "outputId": "b8e2f7be-2248-4a4b-ccea-caaa90044aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "(4, 8)\n",
            "tf.Tensor(\n",
            "[[ 1 51 63  1 41 53 39 58]\n",
            " [39 42  0 20 47 57  1 52]\n",
            " [32 53  1 56 43 60 43 50]\n",
            " [54 39 52 63  1 54 47 43]], shape=(4, 8), dtype=int32)\n",
            "targets:\n",
            "(4, 8)\n",
            "tf.Tensor(\n",
            "[[51 63  1 41 53 39 58  6]\n",
            " [42  0 20 47 57  1 52 39]\n",
            " [53  1 56 43 60 43 50  1]\n",
            " [39 52 63  1 54 47 43 41]], shape=(4, 8), dtype=int32)\n",
            "When input is [1] the target is 51\n",
            "When input is [1, 51] the target is 63\n",
            "When input is [1, 51, 63] the target is 1\n",
            "When input is [1, 51, 63, 1] the target is 41\n",
            "When input is [1, 51, 63, 1, 41] the target is 53\n",
            "When input is [1, 51, 63, 1, 41, 53] the target is 39\n",
            "When input is [1, 51, 63, 1, 41, 53, 39] the target is 58\n",
            "When input is [1, 51, 63, 1, 41, 53, 39, 58] the target is 6\n",
            "When input is [39] the target is 42\n",
            "When input is [39, 42] the target is 0\n",
            "When input is [39, 42, 0] the target is 20\n",
            "When input is [39, 42, 0, 20] the target is 47\n",
            "When input is [39, 42, 0, 20, 47] the target is 57\n",
            "When input is [39, 42, 0, 20, 47, 57] the target is 1\n",
            "When input is [39, 42, 0, 20, 47, 57, 1] the target is 52\n",
            "When input is [39, 42, 0, 20, 47, 57, 1, 52] the target is 39\n",
            "When input is [32] the target is 53\n",
            "When input is [32, 53] the target is 1\n",
            "When input is [32, 53, 1] the target is 56\n",
            "When input is [32, 53, 1, 56] the target is 43\n",
            "When input is [32, 53, 1, 56, 43] the target is 60\n",
            "When input is [32, 53, 1, 56, 43, 60] the target is 43\n",
            "When input is [32, 53, 1, 56, 43, 60, 43] the target is 50\n",
            "When input is [32, 53, 1, 56, 43, 60, 43, 50] the target is 1\n",
            "When input is [54] the target is 39\n",
            "When input is [54, 39] the target is 52\n",
            "When input is [54, 39, 52] the target is 63\n",
            "When input is [54, 39, 52, 63] the target is 1\n",
            "When input is [54, 39, 52, 63, 1] the target is 54\n",
            "When input is [54, 39, 52, 63, 1, 54] the target is 47\n",
            "When input is [54, 39, 52, 63, 1, 54, 47] the target is 43\n",
            "When input is [54, 39, 52, 63, 1, 54, 47, 43] the target is 41\n"
          ]
        }
      ],
      "source": [
        "## To be worked on : packaging the code with script with variables for later\n",
        "# Depiction of the chunk(or in here, block)-wise transformation.\n",
        "# Having varied blocksize allows the algorithm to take into account the context for inference purpose\n",
        "\n",
        "tf.random.set_seed(1337) # For reproducibility, to be sure to have consistent random number\n",
        "batch_size = 4 # The number of independent sequences to train in parallel\n",
        "block_size = 8 # The maximum context length for prediction\n",
        "\n",
        "def get_batch(split):\n",
        "    '''\n",
        "    Function to generate a small batch of data of inputs x and targets y\n",
        "    '''\n",
        "\n",
        "    data = data_train if split == 'train' else data_test\n",
        "    # Retrieving batches randomly\n",
        "    ix = tf.random.uniform(shape = (batch_size,),\n",
        "                          maxval = len(data) - block_size,\n",
        "                          dtype = tf.int32)\n",
        "    # Stacking the list of tensors\n",
        "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
        "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "for batch in range(batch_size):       # Batch dimension\n",
        "    for block in range(block_size):   # Time dimension\n",
        "        context = xb[batch, :block+1]\n",
        "        target = yb[batch, block]\n",
        "        print(f'When input is {context.numpy().tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic BigramModel for training"
      ],
      "metadata": {
        "id": "6o2p6veM6AH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1337)\n",
        "# Hyperparameters\n",
        "batch_size = 16 # Independent sequences to process in parallel\n",
        "block_size = 32 # Maximum context length for prediction\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 500\n",
        "n_embed = 64"
      ],
      "metadata": {
        "id": "dQiHX1ws1FPy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s5cA7HEusdrS",
        "outputId": "5caa8349-64b9-4c96-d786-cd9e812d177f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 65)\n",
            "4.1692066\n",
            "\n",
            "OfrXJkURRD!zvrG&onkf!\n",
            "Sx3;VgugnVa\n",
            "ZZ;zy&fJYPHUsg$pq:SpKDArjSlKgsyFYefclQs?YLBRYAT;HTO,V?c'P?kE$rrseb\n"
          ]
        }
      ],
      "source": [
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = block size = sequence length\n",
        "        C : Channel = vocab size = number of classes\n",
        "        '''\n",
        "        logits = self.token_embedding_table(idx)  # Replacing embedding to the indices\n",
        "\n",
        "        if targets is None: # If target is not provided\n",
        "            loss = None\n",
        "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # Focus only on the last time step (i.e. history is not being used)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # One sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
        "\n",
        "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_basic = BigramLanguageModel(vocab_size)\n",
        "\n",
        "logits, loss = model_basic.call(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss.numpy())\n",
        "\n",
        "print(decode(model_basic.generate(idx=tf.zeros((1, 1), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an optimizer, and training the model"
      ],
      "metadata": {
        "id": "OzC_WFoR4GjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2GRgIwnCsdrS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def estimate_loss(model):\n",
        "  '''\n",
        "  Function to average up the loss in multiple batches for both splits\n",
        "  '''\n",
        "  output = {}\n",
        "  model.trainable = False # Setting the model to evaluation phase\n",
        "  for split in ['train','val']:\n",
        "      losses = []\n",
        "      for _ in range(eval_iters):\n",
        "          X, Y = get_batch(split)\n",
        "          logits, loss = model.call(X,Y)\n",
        "          losses.append(loss)\n",
        "      output[split] = tf.reduce_mean(losses)\n",
        "  model.trainable = True # Setting the model back to training phase\n",
        "  return output\n",
        "\n",
        "def model_train(model):\n",
        "    optimizer = Adam(learning_rate = learning_rate)\n",
        "    for step in tf.range(max_iters):\n",
        "        if step % eval_iters == 0:\n",
        "            losses = estimate_loss(model)\n",
        "            if step != 0:\n",
        "                end = time.time()\n",
        "                print(f\"Step {step}\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f} | time {(end-start)//60:.0f} min {(end-start)%60:.0f} seconds\")\n",
        "                start = time.time()\n",
        "            else:\n",
        "                print(f\"Step {step}\\t\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f}\")\n",
        "                start = time.time()\n",
        "\n",
        "        # Sample a batch of data\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # Evaluate the loss and update parameters\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits, loss = model(xb,yb)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    print(f'Final Loss: {loss.numpy()}')\n",
        "\n",
        "def model_generate():\n",
        "    # Generate a sequence\n",
        "    print('\\n\\n======================= Generated Sequence =======================')\n",
        "    idx = tf.zeros((1, 1), dtype=tf.int32)\n",
        "    generated_sequence = model.generate(idx, max_new_tokens=500).numpy()\n",
        "    print(decode(generated_sequence[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ziuASsv3sdrT",
        "outputId": "5463a67d-b10c-4bfc-e701-655e757b064c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\t\t train loss 4.1747 | val loss 4.1746\n",
            "Step 500\t train loss 3.0823 | val loss 3.0923 | time 0 min 55 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-81a25b390177>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_basic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-dd2fbdee2b51>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_iters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-dd2fbdee2b51>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e9683a802271>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mChannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         '''\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replacing embedding to the indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# If target is not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m                 \u001b[0;31m# Change the layout for the layer output if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0;31m# This is useful for relayout intermediate tensor in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"int32\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/numpy.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(x, indices, axis)\u001b[0m\n\u001b[1;32m   4873\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4874\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4875\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(x, indices, axis)\u001b[0m\n\u001b[1;32m   1946\u001b[0m     indices = tf.where(\n\u001b[1;32m   1947\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1948\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1949\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m       \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    725\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mout_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9605\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9606\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9607\u001b[0;31m       return shape_eager_fallback(\n\u001b[0m\u001b[1;32m   9608\u001b[0m           input, out_type=out_type, name=name, ctx=_ctx)\n\u001b[1;32m   9609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape_eager_fallback\u001b[0;34m(input, out_type, name, ctx)\u001b[0m\n\u001b[1;32m   9635\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9636\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"out_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9637\u001b[0;31m   _result = _execute.execute(b\"Shape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m   9638\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   9639\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_train(model_basic)\n",
        "model_generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt-q5K1BsdrT"
      },
      "source": [
        "## The mathematical trick in self-attention\n",
        "Below present different ways of calculating weighted aggregation of a matrix, from beginning of the block in each batch, up to the 't'th token. The results of the four approaches are the same"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens learning from previous context, by calculating average up to 't'th token\n",
        "B,T,C = 4,8,2 # Batch, Time, Channels\n",
        "x = tf.random.uniform(shape=(B, T,C))"
      ],
      "metadata": {
        "id": "p1Ypwptzhc6a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 1: Basic"
      ],
      "metadata": {
        "id": "7iqndYlwH3kp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-eOjLoiqsdrT"
      },
      "outputs": [],
      "source": [
        "xbow = tf.zeros((B,T,C)) # Defining a bag of words\n",
        "for b in range (B):\n",
        "    for t in range (T):\n",
        "        xprev = x[b, :t+1] # (t, C) Batch, including the 't'th token\n",
        "        xbow = xbow.numpy()  # Convert xbow to numpy array to support assignment\n",
        "        xbow[b, t] = tf.reduce_mean(xprev, axis=0).numpy()  # Calculate mean and assign to xbow\n",
        "        xbow = tf.convert_to_tensor(xbow)  # Convert back to tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vamUFUHssdrT"
      },
      "source": [
        "### Version 2: Vectorizing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HwFOca6CsdrT",
        "outputId": "792914a6-e576-4fb4-c2f9-0f034d7fb97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "w = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
        "w = w / tf.math.reduce_sum(w, axis = 1, keepdims = True) # Low triangular matrix for calculating average weights\n",
        "\n",
        "xbow2 = w @ x # (B, T, T) @ (B , T, C) --> (B, T, C)\n",
        "tf.experimental.numpy.allclose(xbow,xbow2).numpy() # Checking whether xbow == xbow2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 3: Using softmax"
      ],
      "metadata": {
        "id": "auCbs_uBIBei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mloGBUAzsdrT",
        "outputId": "63cfa72d-94dc-4adb-9b32-bfbf5b4a89fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
        "w = tf.zeros((T,T))\n",
        "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
        "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
        "xbow3 = w @ x\n",
        "tf.experimental.numpy.allclose(xbow,xbow3).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 4: Self-attention\n",
        "\n",
        "Called self-attention as the key, query and value are generated from the same value (x)\n",
        "\n",
        "Note that key and query weights values are different as"
      ],
      "metadata": {
        "id": "OVt_n-VReoxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention mechanism\n",
        "head_size = 16\n",
        "key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "k = key(x) # Weights adjusted, (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "w = q @ tf.transpose(k, perm=[0,2,1]) # (B, T, 16) @ (B, 16, T) -> (B, T, T), with (T, T) indicating elements compared with every element in the sequence\n",
        "\n",
        "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
        "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
        "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
        "\n",
        "v = value(x)\n",
        "out = w @ v # Using aggregated value instead of the raw x for dimensionality reduction, information extraction\n",
        "out.shape"
      ],
      "metadata": {
        "id": "4UtHbUZCerfL",
        "outputId": "83a40638-7be2-45de-b262-9440fb9b5d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgtNLQkCsdrT"
      },
      "source": [
        "Notes:\n",
        "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over as a set of vectors. This is why we need to positionally encode tokens\n",
        "- Each example across batch dimension is of course processed completely independently and never 'talk' to each other\n",
        "- In an 'encoder' attention block (w = tf.where(tril == 0, float('-inf'), w))code can be omitted, allowing all tokens to communicate. This block here is called a 'decoder' attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- 'Self attention' just means that the keys and values are produced from the same source as queries. In 'Cross-attention', the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- 'Scaled' attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, wei will be unit variance too and softmax will stay diffuses and not saturate too much, Illustration below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_HY8n3sdrT"
      },
      "source": [
        "## Modified BigramModel with self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "c468d4a1-3850-4c18-d7fb-9c99adfddf09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4JRlLv4LFms"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 65)\n",
            "4.173274\n",
            "\n",
            "saTf-Wz-K-?hNk;Yr:r'KUFLHH:QmLbpClI\n",
            "oYwnqePrE\n",
            "!zgz'U:,?ZgzxEjHtgpzAQjGjM&vv.;OBdqFlP qxcwcexWhPKs:$'\n"
          ]
        }
      ],
      "source": [
        "class Head(tf.keras.Model):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(Head, self).__init__()\n",
        "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B,T,C)\n",
        "        q = self.query(x)   # (B,T,C)\n",
        "        # Compute attention scores ('affinities')\n",
        "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
        "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
        "        self.sa_head = Head(n_embed)\n",
        "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = block size = sequence length\n",
        "        C : Channel = vocab size = number of classes\n",
        "        '''\n",
        "        B,T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
        "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
        "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
        "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None: # If target is not provided\n",
        "            loss = None\n",
        "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # Focus only on the last time step (i.e. history is not being used)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # One sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
        "\n",
        "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_sa = BigramLanguageModel(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(model_sa)\n",
        "model_generate()"
      ],
      "metadata": {
        "id": "zqlLK9RT4HBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-headed attention"
      ],
      "metadata": {
        "id": "rqcACneZN9rj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jZNYqeA4sdrT",
        "outputId": "ff965f06-a210-4053-c33e-babf2fba029f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.1746, val loss 4.1745\n",
            "Step 200: train loss 3.1012, val loss 3.1210\n",
            "Step 400: train loss 2.7699, val loss 2.7703\n",
            "Step 600: train loss 2.5751, val loss 2.5762\n",
            "Step 800: train loss 2.5168, val loss 2.5067\n",
            "Step 1000: train loss 2.4686, val loss 2.4885\n",
            "Step 1200: train loss 2.4425, val loss 2.4450\n",
            "Step 1400: train loss 2.4229, val loss 2.4162\n",
            "Step 1600: train loss 2.3848, val loss 2.4026\n",
            "Step 1800: train loss 2.3838, val loss 2.3925\n",
            "Step 2000: train loss 2.3692, val loss 2.3726\n",
            "Step 2200: train loss 2.3655, val loss 2.3786\n",
            "Step 2400: train loss 2.3449, val loss 2.3539\n",
            "Step 2600: train loss 2.3317, val loss 2.3614\n",
            "Step 2800: train loss 2.3375, val loss 2.3406\n",
            "Step 3000: train loss 2.3247, val loss 2.3334\n",
            "Step 3200: train loss 2.3183, val loss 2.3479\n",
            "Step 3400: train loss 2.3050, val loss 2.3269\n",
            "Step 3600: train loss 2.2972, val loss 2.3155\n",
            "Step 3800: train loss 2.2976, val loss 2.3314\n",
            "Step 4000: train loss 2.2903, val loss 2.3336\n",
            "Step 4200: train loss 2.2872, val loss 2.3216\n",
            "Step 4400: train loss 2.2963, val loss 2.3057\n",
            "Step 4600: train loss 2.2901, val loss 2.3209\n",
            "Step 4800: train loss 2.2631, val loss 2.3099\n",
            "Final Loss: 2.1268467903137207\n",
            "===================== Generated Sequence =====================\n",
            "\n",
            "HAAONEXISthapt andve mous, woy Idede orm inon I what ufle won pe youse.\n",
            "Vfare; soor Pof allenes thor yould ans den wsen wop to wad bun, yo\n",
            "Wherk.\n",
            "Wer'st dpow tly hour theyurtutend heptere thime uadea mir ly uie, there dy swer ury habt,\n",
            "The thor ay me aludeito forcut\n",
            "Tha nont that for for, thads Lall you phack sarmy unt bu thith Cis sutie ot the weiod?\n",
            "\n",
            "Whoust to sell.\n",
            "I\n",
            "NIHASY:\n",
            "He Bege gor, foouckn thourear, So mour poree eat llof owand ake Prad sow; sent, nomtirt ten not but'da thighe,\n",
            "W's: you youravaluernor; at,hat the byetent,\n",
            "Lin farwe congt'd whor'ter:\n",
            "Th I bos tave a's agou shave val liet your.\n",
            "Tremy tay, not o bran that of liles fn of uow lived uea to shindke hate sto ene comeefaro wil the vem salte thof an tous songt, thouunsilwatied's. I:\n",
            "Coo of hicie viquere,, yesech ge; in dear:\n",
            "The ued far thown's fe, tholvake,\n",
            "Tof sy I:\n",
            "d cerrar en whearg mes\n",
            "To aricker hill site exedig so su woor arm cing\n",
            "May\n",
            "Onete's hit, arere:\n",
            "Powne Loo, witlen the shovears boomant hopry,\n",
            "Whe tit the:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class Head(tf.keras.Model):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(Head, self).__init__()\n",
        "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B,T,C)\n",
        "        q = self.query(x)   # (B,T,C)\n",
        "        # Compute attention scores ('affinities')\n",
        "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
        "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "# ================================================================== #\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    '''Multiple heads of self-attention in parallel'''\n",
        "\n",
        "    def __init__(self,num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
        "\n",
        "    def call(self, x):\n",
        "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        return out\n",
        "# ================================================================== #\n",
        "\n",
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
        "# ================================================================== #\n",
        "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
        "# ================================================================== #\n",
        "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = block size = sequence length\n",
        "        C : Channel = vocab size = number of classes\n",
        "        '''\n",
        "        B,T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
        "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
        "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
        "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None: # If target is not provided\n",
        "            loss = None\n",
        "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # Focus only on the last time step (i.e. history is not being used)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # One sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
        "\n",
        "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_ma = BigramLanguageModel(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(model_ma)\n",
        "model_generate()"
      ],
      "metadata": {
        "id": "Cn6BDAZj4h7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward"
      ],
      "metadata": {
        "id": "otx96pZaoG7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "tf.random.set_seed(1337)\n",
        "\n",
        "class Head(tf.keras.Model):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(Head, self).__init__()\n",
        "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B,T,C)\n",
        "        q = self.query(x)   # (B,T,C)\n",
        "        # Compute attention scores ('affinities')\n",
        "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
        "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    '''Multiple heads of self-attention in parallel'''\n",
        "\n",
        "    def __init__(self,num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
        "\n",
        "    def call(self, x):\n",
        "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        return out\n",
        "\n",
        "# [==================================================================\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    '''A simple linear layer followed by a non-linearity'''\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(n_embed),\n",
        "            tf.keras.layers.ReLU(),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(tf.keras.layers.Layer):\n",
        "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
        "        super().__init__()\n",
        "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
        "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.sa_head(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x\n",
        "# ==================================================================] #\n",
        "\n",
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
        "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
        "# [================================================================== #\n",
        "        self.blocks = tf.keras.Sequential([\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4),])\n",
        "# ==================================================================] #\n",
        "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = block size = sequence length\n",
        "        C : Channel = vocab size = number of classes\n",
        "        '''\n",
        "        B,T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
        "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
        "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
        "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
        "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None: # If target is not provided\n",
        "            loss = None\n",
        "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # Focus only on the last time step (i.e. history is not being used)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # One sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
        "\n",
        "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_ff = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Bot6EZ1joF3a",
        "outputId": "e84d1e61-1591-4f78-ffc7-8e7f5a9b2844"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.1762, val loss 4.1762\n",
            "Step 200: train loss 3.3105, val loss 3.3460 / time 2.994725445906321 min\n",
            "Step 400: train loss 3.3058, val loss 3.3411 / time 3.0028282324473063 min\n",
            "Step 600: train loss 3.3013, val loss 3.3293 / time 2.960600407918294 min\n",
            "Step 800: train loss 2.9541, val loss 2.9453 / time 2.93562490940094 min\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-606da17262c6>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_BatchMatMulV2\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1887\u001b[0m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m   \u001b[0mgrad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m   \u001b[0mgrad_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_getitem_override.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops_stack\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       packed_begin, packed_end, packed_strides = (\n\u001b[0;32m--> 230\u001b[0;31m           \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m           \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           array_ops_stack.stack(strides))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops_stack.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    714\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    240\u001b[0m       \u001b[0mto_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__tf_tensor__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       ret = (\n\u001b[0;32m--> 242\u001b[0;31m           \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mto_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m           \u001b[0;32melse\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 name=name))\n\u001b[1;32m    607\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_capture_as_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m   def __tf_tensor__(\n\u001b[0m\u001b[1;32m    758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m       ) -> \"Tensor\":\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(model_ma)\n",
        "model_generate()"
      ],
      "metadata": {
        "id": "QRIOtkuy41OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "\n",
        "1) residual\n",
        "\n",
        "2) pre-layer norm (different from the original paper)>> make more series 3"
      ],
      "metadata": {
        "id": "iFBQ1ERDq-iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(tf.keras.Model):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(Head, self).__init__()\n",
        "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B,T,C)\n",
        "        q = self.query(x)   # (B,T,C)\n",
        "        # Compute attention scores ('affinities')\n",
        "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
        "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    '''Multiple heads of self-attention in parallel'''\n",
        "\n",
        "    def __init__(self,num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        self.projection = tf.keras.layers.Dense(n_embed)\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "    def call(self, x):\n",
        "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        out = self.projection(out)\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        return out\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    '''A simple linear layer followed by a non-linearity'''\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = tf.keras.Sequential([\n",
        "# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(tf.keras.layers.Layer):\n",
        "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
        "        super().__init__()\n",
        "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
        "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "# ++++++++++\n",
        "\n",
        "    def call(self, x):\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        # Residual Connections to preserve information, and improve gradient flow\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
        "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
        "# [================================================================== #\n",
        "        self.blocks = tf.keras.Sequential([\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4),\n",
        "            Block(n_embed, n_head=4),\n",
        "            tf.keras.layers.LayerNormalization(axis=-1),\n",
        "            ])\n",
        "# ==================================================================] #\n",
        "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "    def call(self, idx, targets=None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = block size = sequence length\n",
        "        C : Channel = vocab size = number of classes\n",
        "        '''\n",
        "        B,T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
        "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
        "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
        "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
        "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None: # If target is not provided\n",
        "            loss = None\n",
        "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # Focus only on the last time step (i.e. history is not being used)\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # One sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
        "\n",
        "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_opt = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULXjoU5gq-6B",
        "outputId": "cd3b769b-01ef-47cb-d2b7-55041ab58c8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.7659, val loss 4.7509\n",
            "Step 200: train loss 3.1269, val loss 3.1374\n",
            "Step 400: train loss 2.6031, val loss 2.6037\n",
            "Step 600: train loss 2.4571, val loss 2.4562\n",
            "Step 800: train loss 2.3824, val loss 2.3861\n",
            "Step 1000: train loss 2.3018, val loss 2.3103\n",
            "Step 1200: train loss 2.2622, val loss 2.2704\n",
            "Step 1400: train loss 2.2064, val loss 2.2291\n",
            "Step 1600: train loss 2.1699, val loss 2.1966\n",
            "Step 1800: train loss 2.1127, val loss 2.1435\n",
            "Step 2000: train loss 2.0834, val loss 2.1250\n",
            "Step 2200: train loss 2.0512, val loss 2.1090\n",
            "Step 2400: train loss 2.0274, val loss 2.0999\n",
            "Step 2600: train loss 2.0088, val loss 2.0921\n",
            "Step 2800: train loss 1.9751, val loss 2.0646\n",
            "Step 3000: train loss 1.9505, val loss 2.0348\n",
            "Step 3200: train loss 1.9346, val loss 2.0302\n",
            "Step 3400: train loss 1.9152, val loss 2.0030\n",
            "Step 3600: train loss 1.8949, val loss 1.9927\n",
            "Step 3800: train loss 1.8719, val loss 1.9819\n",
            "Step 4000: train loss 1.8403, val loss 1.9677\n",
            "Step 4200: train loss 1.8438, val loss 1.9864\n",
            "Step 4400: train loss 1.8239, val loss 1.9563\n",
            "Step 4600: train loss 1.8040, val loss 1.9616\n",
            "Step 4800: train loss 1.7959, val loss 1.9317\n",
            "Final Loss: 1.8865973949432373\n",
            "===================== Generated Sequence =====================\n",
            "\n",
            "YOK:\n",
            "Prace! Show, recome than jecter,\n",
            "Irent yea? your come up e'drce\n",
            "Pefivous.\n",
            "\n",
            "ESCULE:\n",
            "Hes face glee, some too, here's tear let slat my swelle warrod\n",
            "'Twered XI:\n",
            "Loop were of grevends\n",
            "As wet one aliker oner's ramsage, ill.\n",
            "\n",
            "BANGOULET:\n",
            "I'n seer had as youse\n",
            "criencary, to well supbtear: ret too!\n",
            "\n",
            "DORWUK:\n",
            "Do prayy's we, Were nath as id but deat cop'ss you vildous fear handar\n",
            "walce in 'tarws were crat of Thee:\n",
            "The, heal have to well shee, wor meaven;\n",
            "Henred'd, count not now.\n",
            "Thir necous mother the death fallseter's\n",
            "And tain be joy must fignce: to of gruch gefight;\n",
            "Should Mut be in the boatie.\n",
            "\n",
            "DUKE NIA:\n",
            "I chrow chall heart of Murce, my bid holy:\n",
            "So formes muou, son.\n",
            "\n",
            "SLOULEY:\n",
            "E mazen Mer begutwc, the and heave's with weres!\n",
            "The wisht me, so as, I, windile were,\n",
            "To thy less arcite made the flones!\n",
            "I, ay dantaind, time unce, trunke grorveal,\n",
            "A wer, corceas, a isspelf unvilly have:\n",
            "Tyou father here not how, it\n",
            "Flesere and net you nect\n",
            "A shall: of a ovind-fleers: low hence,\n",
            "A' is was dysers s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_train(model_opt)\n",
        "model_generate()"
      ],
      "metadata": {
        "id": "NABgGbgJ5APJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scaling up the model"
      ],
      "metadata": {
        "id": "pKAqqRfJvx1p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SvlxvR-Zv7cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYe3qjEKsdrT",
        "outputId": "b75d3355-72ce-49d4-b568-ae91d8910b22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([4, 8, 16])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Video from 1:00:00. Need to recheck how self-attention functions\n",
        "\n",
        "# Version 4: self-attention\n",
        "tf.random.set_seed(1337)\n",
        "B,T,C = 4, 8, 32 # batch, time, channels\n",
        "x = tf.random.normal(shape=(B, T, C))\n",
        "\n",
        "# Single head perform self-attention\n",
        "head_size = 16\n",
        "key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "\n",
        "# All tokens in all positions produce independent key and query\n",
        "k = key(x) # B, T, 16\n",
        "q = query(x) # B, T, 16\n",
        "# Communicating key with query\n",
        "w = q @ tf.transpose(k, perm=[0,2,1]) # (B, T, 16) @ (B, 16, T) -> (B,T,T)\n",
        "\n",
        "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
        "w = tf.zeros((T,T))\n",
        "w = tf.where(tril == 0, float('-inf'), w) # Upper triangular masking, indicating future bow cannot communicate with the past\n",
        "w = tf.nn.softmax(w, axis = -1) # normalizing the weight matrix\n",
        "\n",
        "v = value(x)\n",
        "out = w @ v\n",
        "#out = w @ x # K : private information\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpNuGRGNsdrT",
        "outputId": "4451c7df-e121-40d6-b17a-0fbf8ca6df3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.       0.       0.       0.       0.       0.       0.       0.      ]\n",
            " [0.5      0.5      0.       0.       0.       0.       0.       0.      ]\n",
            " [0.333333 0.333333 0.333333 0.       0.       0.       0.       0.      ]\n",
            " [0.25     0.25     0.25     0.25     0.       0.       0.       0.      ]\n",
            " [0.2      0.2      0.2      0.2      0.2      0.       0.       0.      ]\n",
            " [0.166667 0.166667 0.166667 0.166667 0.166667 0.166667 0.       0.      ]\n",
            " [0.142857 0.142857 0.142857 0.142857 0.142857 0.142857 0.142857 0.      ]\n",
            " [0.125    0.125    0.125    0.125    0.125    0.125    0.125    0.125   ]]\n"
          ]
        }
      ],
      "source": [
        "# Verifying whether the row sum of weights equal 1\n",
        "import numpy as np\n",
        "# Convert TensorFlow tensor to NumPy array\n",
        "w_np = w.numpy()\n",
        "\n",
        "# Set NumPy print options to suppress scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Print the tensor\n",
        "print(w_np.round(6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvl5KXLEsdrT"
      },
      "outputs": [],
      "source": [
        "q = tf.random.normal((B, T, head_size))\n",
        "k = tf.random.normal((B, T, head_size))\n",
        "\n",
        "# Calculate the weights\n",
        "wei = q @ tf.transpose(k, perm=[0, 2, 1]) * (head_size ** -0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRNjY0ThsdrU",
        "outputId": "c643ec9b-19d6-4910-d9ad-f0e109947de9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9939072"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.math.reduce_variance(k).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bRBh_KYsdrU",
        "outputId": "9fea4599-889c-42d9-fe8f-ec924e6c9f46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9280848"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.math.reduce_variance(q).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZBnGnDsdrU",
        "outputId": "017f049b-e6ea-4649-af35-da99732ae051"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9270358"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.math.reduce_variance(wei).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rF0c_GmsdrU",
        "outputId": "2a0b7a10-812b-4293-a39e-4f43f18d652d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([4, 8, 32])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZQxtMTmsdrU",
        "outputId": "fde35067-2997-4444-9eaa-fc7dd052ff24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16384, 65)\n",
            "4.9176507\n",
            "Step 0: train loss 4.9358, val loss 4.9593\n",
            "Step 200: train loss 3.0522, val loss 3.0692\n",
            "Step 400: train loss 2.7568, val loss 2.7682\n",
            "Step 600: train loss 2.6052, val loss 2.6109\n",
            "Step 800: train loss 2.5359, val loss 2.5471\n",
            "Step 1000: train loss 2.5138, val loss 2.5287\n",
            "Step 1200: train loss 2.4898, val loss 2.5081\n",
            "Step 1400: train loss 2.5138, val loss 2.5282\n",
            "Step 1600: train loss 2.5075, val loss 2.5315\n",
            "Step 1800: train loss 2.4910, val loss 2.5144\n",
            "Final Loss: 2.4965360164642334\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ORO tharthy,\n",
            "Ost veapy t ours,\n",
            "Auladeane wese t\n",
            "Ol thlent ug he s?\n",
            "In,\n",
            "ncordllder urild:\n",
            "nouye aveet\n"
          ]
        }
      ],
      "source": [
        "head_size = 6\n",
        "max_iters = 2000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "eval_iters = 200\n",
        "n_embed = 384\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "dropout = 0.3\n",
        "\n",
        "n_layer = 3\n",
        "\n",
        "class Head(tf.keras.Model):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(Head, self).__init__()\n",
        "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
        "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)     # (B,T,C)\n",
        "        q = self.query(x)   # (B,T,C)\n",
        "        # compute attention scores ('affinities')\n",
        "        wei = tf.matmul(q, k, transpose_b=True) * (C ** -0.5) # (B,T,C) @ (B,C,T ) -> (B,T,T))\n",
        "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part\n",
        "        wei = tf.nn.softmax(wei, axis = -1)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = tf.matmul(wei, v) # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    '''A simple linear layer followed by a non-linearity'''\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
        "            tf.keras.layers.Dropout(dropout),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    '''Multiple heads of self-attention in parallel'''\n",
        "\n",
        "    def __init__(self,num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
        "        self.projection = tf.keras.layers.Dense(n_embed)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
        "        out = self.dropout(self.projection(out))\n",
        "        return out\n",
        "\n",
        "class Block(tf.keras.layers.Layer):\n",
        "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        '''Initializing embedding layer, which maps integer indices to\n",
        "        dense vectors of vocab size'''\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
        "        self.blocks = tf.keras.Sequential([Block(n_embed = n_embed, n_head = 4) for _ in range(n_layer)])\n",
        "        self.ln_f = tf.keras.layers.LayerNormalization(axis=-1) # final layer normalization\n",
        "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "    def call(self, idx, targets = None):\n",
        "        '''Method for loss calculation, based on idx (input token indices) and\n",
        "        target (target token indices)\n",
        "        B : Batch size\n",
        "        T : Time = sequence length = block size\n",
        "        C : Channel = number of classes = vocab size\n",
        "        '''\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        position_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T, C)\n",
        "        x = token_emb + position_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B, T, C)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Reshaping the tensor so that it's compatible with categorical cross entropy\n",
        "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
        "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
        "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        '''\n",
        "        Text generating method\n",
        "        '''\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
        "            # sample prediction from the distribution\n",
        "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int32)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "logits, loss = model.call(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss.numpy())\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "for step in tf.range(max_iters):\n",
        "    if step % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits, loss = model(xb,yb)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "print(f'Final Loss: {loss.numpy()}')\n",
        "\n",
        "print(decode(model.generate(idx=tf.zeros((1, block_size), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5VOZrv4sdrV",
        "outputId": "40858cc0-b0d8-4562-e545-74bf752495e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MOF:\n",
            "HBRDUEENOLIA:\n",
            "ININWAN:\n",
            "AUROLAOLI: od the O:\n",
            "ARCut-\n",
            "Fo lelle h aver t rstwathit bellly poenly ll\n"
          ]
        }
      ],
      "source": [
        "print(decode(model.generate(idx=tf.zeros((1, block_size), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}