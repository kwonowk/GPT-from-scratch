{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  1953k      0 --:--:-- --:--:-- --:--:-- 1952k\n"
     ]
    }
   ],
   "source": [
    "# Downloading tinyshakesphere for training\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspecting the tinyshakespeare text file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the text file\n",
    "with open('tinyshakespeare.txt','r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1115394 characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(text)} characters in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 1000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters (including white space): 65\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Identifying the number of unique characters contained in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters (including white space): {vocab_size}{''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic mapping between characters to integers\n",
    "\n",
    "More sophisticated examples include Google's SentencePiece and OpenAI's tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 52, 1, 42, 47, 45, 47, 58, 57]\n",
      "Shakespeare in digits\n"
     ]
    }
   ],
   "source": [
    "# Assigning numbers to each characters to encode the characters to integers\n",
    "ctoi = {char : num for num, char in enumerate(chars)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "print(encode('Shakespeare in digits'))\n",
    "\n",
    "# Reversely, decode integers back to characters\n",
    "itoc = {num : char for num, char in enumerate(chars)}\n",
    "decode = lambda l : ''.join([itoc[i] for i in l])\n",
    "print(decode(encode('Shakespeare in digits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 20:32:55.658850: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-18 20:32:55.688534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 20:32:56.745198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the total text. Adapted the code to work with Tensorflow instead of pytorch\n",
    "import tensorflow as tf\n",
    "data = tf.convert_to_tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data : 1003854\n",
      "Length of test data : 111540\n"
     ]
    }
   ],
   "source": [
    "# Train and validation split sets, with 9:1 ratio\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "print(f'Length of train data : {len(data_train)}\\nLength of test data : {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int32)\n",
      "Input : [18], Output : 47\n",
      "Input : [18 47], Output : 56\n",
      "Input : [18 47 56], Output : 57\n",
      "Input : [18 47 56 57], Output : 58\n",
      "Input : [18 47 56 57 58], Output : 1\n",
      "Input : [18 47 56 57 58  1], Output : 15\n",
      "Input : [18 47 56 57 58  1 15], Output : 47\n",
      "Input : [18 47 56 57 58  1 15 47], Output : 58\n"
     ]
    }
   ],
   "source": [
    "# Starting with block_size implementation\n",
    "block_size = 8\n",
    "print(data_train[:block_size + 1])\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Input : {context}, Output : {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1003854,), dtype=int32, numpy=array([18, 47, 56, ..., 43, 56, 43], dtype=int32)>]\n",
      "tf.Tensor(\n",
      "[[ 1 51 63  1 41 53 39 58]\n",
      " [39 42  0 20 47 57  1 52]\n",
      " [32 53  1 56 43 60 43 50]\n",
      " [54 39 52 63  1 54 47 43]], shape=(4, 8), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[51 63  1 41 53 39 58  6]\n",
      " [42  0 20 47 57  1 52 39]\n",
      " [53  1 56 43 60 43 50  1]\n",
      " [39 52 63  1 54 47 43 41]], shape=(4, 8), dtype=int32)\n",
      "When input is [1] the target is 51\n",
      "When input is [1, 51] the target is 63\n",
      "When input is [1, 51, 63] the target is 1\n",
      "When input is [1, 51, 63, 1] the target is 41\n",
      "When input is [1, 51, 63, 1, 41] the target is 53\n",
      "When input is [1, 51, 63, 1, 41, 53] the target is 39\n",
      "When input is [1, 51, 63, 1, 41, 53, 39] the target is 58\n",
      "When input is [1, 51, 63, 1, 41, 53, 39, 58] the target is 6\n",
      "When input is [39] the target is 42\n",
      "When input is [39, 42] the target is 0\n",
      "When input is [39, 42, 0] the target is 20\n",
      "When input is [39, 42, 0, 20] the target is 47\n",
      "When input is [39, 42, 0, 20, 47] the target is 57\n",
      "When input is [39, 42, 0, 20, 47, 57] the target is 1\n",
      "When input is [39, 42, 0, 20, 47, 57, 1] the target is 52\n",
      "When input is [39, 42, 0, 20, 47, 57, 1, 52] the target is 39\n",
      "When input is [32] the target is 53\n",
      "When input is [32, 53] the target is 1\n",
      "When input is [32, 53, 1] the target is 56\n",
      "When input is [32, 53, 1, 56] the target is 43\n",
      "When input is [32, 53, 1, 56, 43] the target is 60\n",
      "When input is [32, 53, 1, 56, 43, 60] the target is 43\n",
      "When input is [32, 53, 1, 56, 43, 60, 43] the target is 50\n",
      "When input is [32, 53, 1, 56, 43, 60, 43, 50] the target is 1\n",
      "When input is [54] the target is 39\n",
      "When input is [54, 39] the target is 52\n",
      "When input is [54, 39, 52] the target is 63\n",
      "When input is [54, 39, 52, 63] the target is 1\n",
      "When input is [54, 39, 52, 63, 1] the target is 54\n",
      "When input is [54, 39, 52, 63, 1, 54] the target is 47\n",
      "When input is [54, 39, 52, 63, 1, 54, 47] the target is 43\n",
      "When input is [54, 39, 52, 63, 1, 54, 47, 43] the target is 41\n"
     ]
    }
   ],
   "source": [
    "## To be worked on : packaging the code with script with variables for later\n",
    "# Depiction of the chunk(or in here, block)-wise transformation.\n",
    "# Having varied blocksize allows the algorithm to take into account the context for inference purpose\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(1337) # To be sure to have consistent random number\n",
    "batch_size = 4 # The number of independent sequences to train in parallel\n",
    "block_size = 8 # The maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    # Retrieving batches randomly\n",
    "    ix = tf.random.uniform(shape = (batch_size,),\n",
    "                           maxval = len(data) - block_size,\n",
    "                           dtype = tf.int32)\n",
    "    # Stacking the list of tensors\n",
    "    print([data])\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb)\n",
    "print(yb)\n",
    "for batch in range(batch_size):\n",
    "    for block in range(block_size):\n",
    "        context = xb[batch, :block+1]\n",
    "        target = yb[batch, block]\n",
    "        print(f'When input is {context.numpy().tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "# A function to average up the loss in multiple batches for both splits\n",
    "@tf.function\n",
    "def estimate_loss():\n",
    "    output = {}\n",
    "    model.trainable = False # Setting the model to evaluation phase\n",
    "    for split in ['train','val']:\n",
    "        print(f'split : {split}')\n",
    "        losses = tf.zeros(eval_iters, dtype=tf.float32)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model.call(X,Y)\n",
    "            losses = tf.tensor_scatter_nd_add(losses, [[k]], [loss])\n",
    "        output[split] = tf.reduce_mean(losses)\n",
    "    model.trainable = True # Setting the model back to training phase\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "4.169551\n",
      "\n",
      "sZTe-Xz-L-?hNk?Yr:r'KUFLHH:QlLboClI\n",
      "oYwnqePrE\n",
      "!zgz'T:,?ZgzxEjHtfpzAQjGjM&vv.;OBdqFlQ qxcwcexWhPKs:$'\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = sequence length = block size\n",
    "        C : Channel = number of classes = vocab size\n",
    "        '''\n",
    "        logits = self.token_embedding_table(idx)  # Replacing embedding to the indices\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshaping the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64)\n",
    "\n",
    "#            idx_next = tf.random.categorical(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.call(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.numpy())\n",
    "\n",
    "print(decode(model.generate(idx=tf.zeros((1, 1), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split : train\n",
      "[<tf.Tensor: shape=(1003854,), dtype=int32, numpy=array([18, 47, 56, ..., 43, 56, 43], dtype=int32)>]\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_24041/2312298259.py\", line 12, in estimate_loss  *\n        X, Y = get_batch(split)\n    File \"/tmp/ipykernel_24041/3300963596.py\", line 19, in get_batch  *\n        x = tf.stack([data[i:i+block_size] for i in ix])\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_24041/2312298259.py\", line 12, in estimate_loss  *\n        X, Y = get_batch(split)\n    File \"/tmp/ipykernel_24041/3300963596.py\", line 19, in get_batch  *\n        x = tf.stack([data[i:i+block_size] for i in ix])\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in tf.range(5000):\n",
    "    print\n",
    "    if step % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb,yb)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print(f'Final Loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VC?zKiUZ-ZEHd-s!Coe'u'r\n",
      "FgsVSUnEhiHN3Ru,3w\n",
      "S:YFEwO&Q;IFxTbni d!bI.UanpCMSqbd-Ac3dRhZn$nNpMhjl3!&yC XuM3Q&aZbhN3kwxUfjlnSChN ojKWV3&JKq,eudY&m-sOM p$nAoVFw;euzTN AAfz.dI$PmGFlEJwe'ZOMdokv$kqwUpC?urj3\n",
      ";XhpzNaZoaY&NH,qr&pbDK,,Wj,aj C'vo:ue?IpEuxkvzuX&eW\n",
      ",XdwB!tPK,Qg'SDIDPp?,lptv'AH t;v$f;OpdPrBur,rMRNf-EA.HRtuwpebAk\n",
      "zkTxFvo,$.DwN;G!XpLU!fbW$GldIJQfm ggRGdaZcDBURx3sJSQ3Z;O IIYWeDl$zXIRSSyW&kjDy-u$r\n",
      "cGFulXTXn?Lj\n",
      "Dmsp.Su,kP'\n",
      "Cc,E,pE'v-Uwide!jG;a$PlByCYj:jhJJpUmYYvV-qAdHZjNNqOBN&mb;hpDxbnNlEXIZWj'IOEBtTav-ESKCbuad.cF:qu';!S&M Wya\n",
      "hVs:HDPVNpQsW3m!bzpT3sW&HP&?tjblmsxfxObE$3rd\n",
      "zyoPUSBEp'LWubU-$U:VSz-Aw\n",
      "33C j:&aqUEQIJZyRDRLbP.FN.bLBUIUXh:RROiemtKg!m3WSmUIMIXq-nVMHo?$y'K-hbON& jkIP-ww,ee?OxwsfsuXJDrdmxEvYWQ3h!OjO!GYTuMVss!udMKw--IHjXe?'lrRe?:gTX\n",
      ":qwqMI!UkjlxEeXYjRZ 3 'xhP,HqP!QfRtdnR\n",
      "xbnsuKJsJZF\n",
      "U\n",
      "lpZ-\n",
      "GRqQiLPRtvEEFiXHC?gikn$ODErqDcffhDow&-wCj-cHuEVS'k?3ViJD'ct,NuUjtWo-cw\n",
      "aAvQyidTAyyeRZ!e:odF.uM DYRxYTbN.&Qibd. CGcx3fTIxfoBPKSWVSf'AWuL$RNkegx ;san?s;vxIzwfKw&'sl:SBvqTWJzf:FFJUHwlx\n",
      "VSQs,GmBM:qrSH?b\n"
     ]
    }
   ],
   "source": [
    "# Generate a sequence\n",
    "idx = tf.zeros((1, 1), dtype=tf.int32)\n",
    "generated_sequence = model.generate(idx, max_new_tokens=1000).numpy()\n",
    "print(decode(generated_sequence[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
