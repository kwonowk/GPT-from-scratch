{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmBdax23tCen"
   },
   "source": [
    "### * While the original source code is written in Pytorch, the below code is adapted to Tensorflow.\n",
    "\n",
    "- GPU utilization not enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7ep-bhusdrP"
   },
   "source": [
    "# 1. Preparing the tinyshakespeare text file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73HKWQbFsdrO",
    "outputId": "34a30494-0117-41ad-fbbd-1e406f800916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2264k      0 --:--:-- --:--:-- --:--:-- 2269k\n"
     ]
    }
   ],
   "source": [
    "# Downloading tinyshakesphere for training\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-iatUv1sdrQ",
    "outputId": "e5589b95-a8ec-410e-8568-aae11607bc6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1115394 characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the text file\n",
    "with open('tinyshakespeare.txt','r') as file:\n",
    "    text = file.read()\n",
    "print(f'There are {len(text)} characters in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpDFL78AsdrR",
    "outputId": "8248245e-4a32-437c-ffa4-2000db4e1d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 1000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOQjexVqsdrR",
    "outputId": "40af24c8-49a4-4827-cc83-8474e8d485b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters (including white space): 65\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Identifying the number of unique characters contained in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters (including white space): {vocab_size}{''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55wnbME6sdrR"
   },
   "source": [
    "# 2. Basic mapping between characters to integers\n",
    "\n",
    "Tokenizing at the character-level.\n",
    "\n",
    "More sophisticated examples of word encoding include Google's SentencePiece and OpenAI's tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq5eWRmEsdrR",
    "outputId": "9c3f50cf-39b1-4dc7-92aa-c1b14b98d46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 52, 1, 42, 47, 45, 47, 58, 57]\n",
      "Shakespeare in digits\n"
     ]
    }
   ],
   "source": [
    "# Assigning numbers to each characters to encode the characters to integers\n",
    "ctoi = {char : num for num, char in enumerate(chars)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "print(encode('Shakespeare in digits'))\n",
    "\n",
    "# Reversely, decode integers back to characters\n",
    "itoc = {num : char for num, char in enumerate(chars)}\n",
    "decode = lambda l : ''.join([itoc[i] for i in l])\n",
    "print(decode(encode('Shakespeare in digits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6HH69LpsdrR",
    "outputId": "0ebe1d05-56f7-4e6a-9219-cbab913e5836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 10:13:15.895501: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 10:13:16.132293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 10:13:17.920698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the total text\n",
    "import tensorflow as tf\n",
    "data = tf.convert_to_tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XHVnVZXsdrR",
    "outputId": "10eee96f-f43d-4a51-ad77-3bba9643db65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data : 1003854\n",
      "Length of test data : 111540\n"
     ]
    }
   ],
   "source": [
    "# Train and validation split sets, with 9:1 ratio\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "print(f'Length of train data : {len(data_train)}\\nLength of test data : {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT2C2TlIsdrS",
    "outputId": "9eb40612-cdbe-4c4c-ace2-d4693c8253eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int32)\n",
      "Input : [18], Output : 47\n",
      "Input : [18 47], Output : 56\n",
      "Input : [18 47 56], Output : 57\n",
      "Input : [18 47 56 57], Output : 58\n",
      "Input : [18 47 56 57 58], Output : 1\n",
      "Input : [18 47 56 57 58  1], Output : 15\n",
      "Input : [18 47 56 57 58  1 15], Output : 47\n",
      "Input : [18 47 56 57 58  1 15 47], Output : 58\n"
     ]
    }
   ],
   "source": [
    "# Starting with block_size implementation\n",
    "block_size = 8                            # Context length\n",
    "print(data_train[:block_size + 1])\n",
    "x = data_train[:block_size]               # Initial block-size\n",
    "y = data_train[1:block_size+1]            # Next block-size\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Input : {context}, Output : {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpT9xJShsdrS",
    "outputId": "b8e2f7be-2248-4a4b-ccea-caaa90044aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 1 51 63  1 41 53 39 58]\n",
      " [39 42  0 20 47 57  1 52]\n",
      " [32 53  1 56 43 60 43 50]\n",
      " [54 39 52 63  1 54 47 43]], shape=(4, 8), dtype=int32)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[51 63  1 41 53 39 58  6]\n",
      " [42  0 20 47 57  1 52 39]\n",
      " [53  1 56 43 60 43 50  1]\n",
      " [39 52 63  1 54 47 43 41]], shape=(4, 8), dtype=int32)\n",
      "When input is [1] the target is 51\n",
      "When input is [1, 51] the target is 63\n",
      "When input is [1, 51, 63] the target is 1\n",
      "When input is [1, 51, 63, 1] the target is 41\n",
      "When input is [1, 51, 63, 1, 41] the target is 53\n",
      "When input is [1, 51, 63, 1, 41, 53] the target is 39\n",
      "When input is [1, 51, 63, 1, 41, 53, 39] the target is 58\n",
      "When input is [1, 51, 63, 1, 41, 53, 39, 58] the target is 6\n",
      "When input is [39] the target is 42\n",
      "When input is [39, 42] the target is 0\n",
      "When input is [39, 42, 0] the target is 20\n",
      "When input is [39, 42, 0, 20] the target is 47\n",
      "When input is [39, 42, 0, 20, 47] the target is 57\n",
      "When input is [39, 42, 0, 20, 47, 57] the target is 1\n",
      "When input is [39, 42, 0, 20, 47, 57, 1] the target is 52\n",
      "When input is [39, 42, 0, 20, 47, 57, 1, 52] the target is 39\n",
      "When input is [32] the target is 53\n",
      "When input is [32, 53] the target is 1\n",
      "When input is [32, 53, 1] the target is 56\n",
      "When input is [32, 53, 1, 56] the target is 43\n",
      "When input is [32, 53, 1, 56, 43] the target is 60\n",
      "When input is [32, 53, 1, 56, 43, 60] the target is 43\n",
      "When input is [32, 53, 1, 56, 43, 60, 43] the target is 50\n",
      "When input is [32, 53, 1, 56, 43, 60, 43, 50] the target is 1\n",
      "When input is [54] the target is 39\n",
      "When input is [54, 39] the target is 52\n",
      "When input is [54, 39, 52] the target is 63\n",
      "When input is [54, 39, 52, 63] the target is 1\n",
      "When input is [54, 39, 52, 63, 1] the target is 54\n",
      "When input is [54, 39, 52, 63, 1, 54] the target is 47\n",
      "When input is [54, 39, 52, 63, 1, 54, 47] the target is 43\n",
      "When input is [54, 39, 52, 63, 1, 54, 47, 43] the target is 41\n"
     ]
    }
   ],
   "source": [
    "## To be worked on : packaging the code with script with variables for later\n",
    "# Depiction of the chunk(or in here, block)-wise transformation.\n",
    "# Having varied blocksize allows the algorithm to take into account the context for inference purpose\n",
    "\n",
    "tf.random.set_seed(1337) # For reproducibility, to be sure to have consistent random number\n",
    "batch_size = 4 # The number of independent sequences to train in parallel\n",
    "block_size = 8 # The maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    '''\n",
    "    Function to generate a small batch of data of inputs x and targets y\n",
    "    '''\n",
    "\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    # Retrieving batches randomly\n",
    "    ix = tf.random.uniform(shape = (batch_size,),\n",
    "                          maxval = len(data) - block_size,\n",
    "                          dtype = tf.int32)\n",
    "    # Stacking the list of tensors\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "for batch in range(batch_size):       # Batch dimension\n",
    "    for block in range(block_size):   # Time dimension\n",
    "        context = xb[batch, :block+1]\n",
    "        target = yb[batch, block]\n",
    "        print(f'When input is {context.numpy().tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o2p6veM6AH1"
   },
   "source": [
    "## Basic BigramModel for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQiHX1ws1FPy"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "# Hyperparameters\n",
    "batch_size = 16 # Independent sequences to process in parallel\n",
    "block_size = 32 # Maximum context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500 # How often evaluate the loss\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200 # How many batches to use to compute loss\n",
    "n_embed = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "zEw2ERgM6KwM",
    "outputId": "a1e968b0-b417-41ac-b5c3-2724e48c0cfe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Train loss, Val loss, Time (min), Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "gpt_results = pd.DataFrame(columns=['Model', 'Train loss', 'Val loss', 'Time (min)', 'Text'])\n",
    "gpt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5cA7HEusdrS",
    "outputId": "3b8f4587-7569-4015-db0d-3700fdd65235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "4.1756525\n",
      "\n",
      "sZTe-Xz-L-?hNl;Yr:r'KUFLHH:QmLbpClI\n",
      "oYwnqePrE\n",
      "!zgz'U:,?ZgzxEjItfpzAQjGjM&vv.;OBdqFlP qxdwcexXhPKs:$'\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        logits = self.token_embedding_table(idx)  # Replacing embedding to the indices\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_basic = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = model_basic.call(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.numpy())\n",
    "\n",
    "print(decode(model_basic.generate(idx=tf.zeros((1, 1), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzC_WFoR4GjM"
   },
   "source": [
    "### Creating an optimizer, and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GRgIwnCsdrS"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def estimate_loss(model):\n",
    "  '''\n",
    "  Function to average up the loss in multiple batches for both splits\n",
    "  '''\n",
    "  output = {}\n",
    "  model.training = False # Setting the model to evaluation phase\n",
    "  for split in ['train','val']:\n",
    "      losses = []\n",
    "      for _ in range(eval_iters):\n",
    "          X, Y = get_batch(split)\n",
    "          logits, loss = model.call(X,Y)\n",
    "          losses.append(loss)\n",
    "      output[split] = tf.reduce_mean(losses)\n",
    "  model.training = True # Setting the model back to training phase\n",
    "  return output\n",
    "\n",
    "def model_train(model, label):\n",
    "    start_train = time.time()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    for step in tf.range(1, max_iters+1):\n",
    "        if (step % eval_interval == 0) or (step == 1):\n",
    "            losses = estimate_loss(model)\n",
    "            if step != 1:\n",
    "                end_int = time.time()\n",
    "                print(f\"Step {step}\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f} | time {(end_int-start_int)//60:.0f} min {(end_int-start_int)%60:.0f} seconds\")\n",
    "                start_int = time.time()\n",
    "            else:\n",
    "                print(f\"Step {step}\\t\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f}\")\n",
    "                start_int = time.time()\n",
    "\n",
    "\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "        # Evaluate the loss and update parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, loss = model(xb,yb)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    end_train = time.time()\n",
    "\n",
    "    # Save result for comparison\n",
    "    global gpt_results\n",
    "    gpt_results = pd.concat([gpt_results,pd.DataFrame({'Model': label,\n",
    "                                                       'Train loss': [round(losses['train'].numpy(),4)],\n",
    "                                                       'Val loss': [round(losses['val'].numpy(),4)],\n",
    "                                                       'Time (min)' : [round((end_train-start_train)/60,0)],\n",
    "                                                       'Text':''})], ignore_index = True)\n",
    "    print(f'Final Loss: {loss.numpy()}')\n",
    "\n",
    "def model_generate(model):\n",
    "    # Generate a sequence\n",
    "    print('\\n======================= Generated Sequence =======================')\n",
    "    idx = tf.zeros((1, 1), dtype=tf.int32)\n",
    "    generated_sequence = model.generate(idx, max_new_tokens=500).numpy()\n",
    "    # Save result for comparison\n",
    "    gpt_results.iloc[-1,4] = decode(generated_sequence[0].tolist())\n",
    "    print(decode(generated_sequence[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "ziuASsv3sdrT",
    "outputId": "a5433902-7e34-4cdf-d0de-08f205a2da78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1714 | val loss 4.1710\n",
      "Step 500\t train loss 3.0852 | val loss 3.0929 | time 0 min 19 seconds\n",
      "Step 1000\t train loss 2.7327 | val loss 2.7406 | time 0 min 19 seconds\n",
      "Step 1500\t train loss 2.5981 | val loss 2.6122 | time 0 min 19 seconds\n",
      "Step 2000\t train loss 2.5461 | val loss 2.5525 | time 0 min 19 seconds\n",
      "Step 2500\t train loss 2.5118 | val loss 2.5228 | time 0 min 19 seconds\n",
      "Step 3000\t train loss 2.4940 | val loss 2.5058 | time 0 min 19 seconds\n",
      "Step 3500\t train loss 2.4790 | val loss 2.4964 | time 0 min 18 seconds\n",
      "Step 4000\t train loss 2.4759 | val loss 2.4906 | time 0 min 19 seconds\n",
      "Step 4500\t train loss 2.4694 | val loss 2.4922 | time 0 min 18 seconds\n",
      "Step 5000\t train loss 2.4714 | val loss 2.4880 | time 0 min 17 seconds\n",
      "Final Loss: 2.486629009246826\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "Sallenl!\n",
      "ND:\n",
      "Hico ixcquthed be sis Gemat wse whe atahe ase MExnd gube aced prenchow, avere ithaiass itooupur deried we t then arme, aceat elllave thiny MAreng url unubak,\n",
      "S: bjxthrhe t kechatre?\n",
      "\n",
      "T:\n",
      "THAngobe hed, fithestha!Mour s t thes\n",
      "O y.\n",
      "MIqunerpe themo vee hilepayoouatheadingick suie t y o'te mond y ithiury s.\n",
      "MIn t ss:\n",
      "CLAHeevizeerouth sspaugh's cher s he mnd.\n",
      "LINTI, cofey with he.\n",
      "CEThe ar co t y isit h sisged, be:\n",
      "\n",
      "Cand, re me, myepe mphorferg.\n",
      "Hand thes hiothergenche avugo, pld hte ive \n"
     ]
    }
   ],
   "source": [
    "model_train(model_basic, 'Basic')\n",
    "model_generate(model_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.4714</td>\n",
       "      <td>2.488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\\nSallenl!\\nND:\\nHico ixcquthed be sis Gemat wse whe atahe ase MExnd gube aced prenchow, avere ithaiass itooupur deried we t then arme, aceat elllave thiny MAreng url unubak,\\nS: bjxthrhe t kechatre?\\n\\nT:\\nTHAngobe hed, fithestha!Mour s t thes\\nO y.\\nMIqunerpe themo vee hilepayoouatheadingick suie t y o'te mond y ithiury s.\\nMIn t ss:\\nCLAHeevizeerouth sspaugh's cher s he mnd.\\nLINTI, cofey with he.\\nCEThe ar co t y isit h sisged, be:\\n\\nCand, re me, myepe mphorferg.\\nHand thes hiothergenche avugo, pld hte ive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  Train loss  Val loss  Time (min)  \\\n",
       "0  Basic      2.4714     2.488         3.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Text  \n",
       "0  \\nSallenl!\\nND:\\nHico ixcquthed be sis Gemat wse whe atahe ase MExnd gube aced prenchow, avere ithaiass itooupur deried we t then arme, aceat elllave thiny MAreng url unubak,\\nS: bjxthrhe t kechatre?\\n\\nT:\\nTHAngobe hed, fithestha!Mour s t thes\\nO y.\\nMIqunerpe themo vee hilepayoouatheadingick suie t y o'te mond y ithiury s.\\nMIn t ss:\\nCLAHeevizeerouth sspaugh's cher s he mnd.\\nLINTI, cofey with he.\\nCEThe ar co t y isit h sisged, be:\\n\\nCand, re me, myepe mphorferg.\\nHand thes hiothergenche avugo, pld hte ive   "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt-q5K1BsdrT"
   },
   "source": [
    "## The mathematical trick in self-attention\n",
    "Below present different ways of calculating weighted aggregation of a matrix, from beginning of the block in each batch, up to the 't'th token. The results of the four approaches are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p1Ypwptzhc6a"
   },
   "outputs": [],
   "source": [
    "# Tokens learning from previous context, by calculating average up to 't'th token\n",
    "B,T,C = 4,8,2 # Batch, Time, Channels\n",
    "x = tf.random.uniform(shape=(B, T,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iqndYlwH3kp"
   },
   "source": [
    "### Version 1: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-eOjLoiqsdrT"
   },
   "outputs": [],
   "source": [
    "xbow = tf.zeros((B,T,C)) # Defining a bag of words\n",
    "for b in range (B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b, :t+1] # (t, C) Batch, including the 't'th token\n",
    "        xbow = xbow.numpy()  # Convert xbow to numpy array to support assignment\n",
    "        xbow[b, t] = tf.reduce_mean(xprev, axis=0).numpy()  # Calculate mean and assign to xbow\n",
    "        xbow = tf.convert_to_tensor(xbow)  # Convert back to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vamUFUHssdrT"
   },
   "source": [
    "### Version 2: Vectorizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwFOca6CsdrT",
    "outputId": "792914a6-e576-4fb4-c2f9-0f034d7fb97c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = w / tf.math.reduce_sum(w, axis = 1, keepdims = True) # Low triangular matrix for calculating average weights\n",
    "\n",
    "xbow2 = w @ x # (B, T, T) @ (B , T, C) --> (B, T, C)\n",
    "tf.experimental.numpy.allclose(xbow,xbow2).numpy() # Checking whether xbow == xbow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auCbs_uBIBei"
   },
   "source": [
    "### Version 3: Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mloGBUAzsdrT",
    "outputId": "63cfa72d-94dc-4adb-9b32-bfbf5b4a89fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = tf.zeros((T,T))\n",
    "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
    "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
    "xbow3 = w @ x\n",
    "tf.experimental.numpy.allclose(xbow,xbow3).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVt_n-VReoxa"
   },
   "source": [
    "### Version 4: Self-attention\n",
    "\n",
    "Called self-attention as the key, query and value are generated from the same value (x)\n",
    "\n",
    "Note that key and query weights values are different as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UtHbUZCerfL",
    "outputId": "83a40638-7be2-45de-b262-9440fb9b5d58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention mechanism\n",
    "head_size = 16\n",
    "key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "k = key(x) # Weights adjusted, (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "w = q @ tf.transpose(k, perm=[0,2,1]) # (B, T, 16) @ (B, 16, T) -> (B, T, T), with (T, T) indicating elements compared with every element in the sequence\n",
    "\n",
    "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
    "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
    "\n",
    "v = value(x)\n",
    "out = w @ v # Using aggregated value instead of the raw x for dimensionality reduction, information extraction\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgtNLQkCsdrT"
   },
   "source": [
    "Notes:\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over as a set of vectors. This is why we need to positionally encode tokens\n",
    "- Each example across batch dimension is of course processed completely independently and never 'talk' to each other\n",
    "- In an 'encoder' attention block (w = tf.where(tril == 0, float('-inf'), w))code can be omitted, allowing all tokens to communicate. This block here is called a 'decoder' attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- 'Self attention' just means that the keys and values are produced from the same source as queries. In 'Cross-attention', the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- 'Scaled' attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, wei will be unit variance too and softmax will stay diffuses and not saturate too much, Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL_HY8n3sdrT"
   },
   "source": [
    "## Modified BigramModel with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4JRlLv4LFms",
    "outputId": "c468d4a1-3850-4c18-d7fb-9c99adfddf09"
   },
   "outputs": [],
   "source": [
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_sa = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zqlLK9RT4HBq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1776 | val loss 4.1773\n",
      "Step 500\t train loss 2.6195 | val loss 2.6206 | time 0 min 37 seconds\n",
      "Step 1000\t train loss 2.4951 | val loss 2.4892 | time 0 min 38 seconds\n",
      "Step 1500\t train loss 2.4367 | val loss 2.4345 | time 0 min 38 seconds\n",
      "Step 2000\t train loss 2.4046 | val loss 2.4170 | time 0 min 37 seconds\n",
      "Step 2500\t train loss 2.3776 | val loss 2.3989 | time 0 min 37 seconds\n",
      "Step 3000\t train loss 2.3649 | val loss 2.3776 | time 0 min 37 seconds\n",
      "Step 3500\t train loss 2.3506 | val loss 2.3590 | time 0 min 37 seconds\n",
      "Step 4000\t train loss 2.3480 | val loss 2.3641 | time 0 min 37 seconds\n",
      "Step 4500\t train loss 2.3382 | val loss 2.3596 | time 0 min 35 seconds\n",
      "Step 5000\t train loss 2.3219 | val loss 2.3593 | time 0 min 37 seconds\n",
      "Final Loss: 2.232243061065674\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "'t gho,\n",
      "Theel mse aplpalint oturaverd uts thel sut thouctre odes amy sabjute, hen n vewnoth arig ar fareatref by tlati cant,\n",
      "Hilds ushirs Anos spor yof ss deen mos pan to ord\n",
      "Thimbewratat alinofr ken rshes;\n",
      "Tor theait ary newer; men fre ungonodug ds?\n",
      "\n",
      "ANID caly I;\n",
      "Mous thee thinto,\n",
      "EFored pigso'd omy spedend the fintetid verownte tim hing\n",
      "QUEEBENDE:\n",
      "And, Lout sowerany tonwe, wat thes mitls,\n",
      "Youm arin culde dstw threwil a gay see,\n",
      "Yous ot gatld arnowe men that thre mein'lll de incarm:\n",
      "That.\n",
      "\n",
      "KMI:\n"
     ]
    }
   ],
   "source": [
    "model_train(model_sa,'Self-attention')\n",
    "model_generate(model_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqcACneZN9rj"
   },
   "source": [
    "## Multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZNYqeA4sdrT",
    "outputId": "ff965f06-a210-4053-c33e-babf2fba029f"
   },
   "outputs": [],
   "source": [
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "# ================================================================== #\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        return out\n",
    "# ================================================================== #\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "# ================================================================== #\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# ================================================================== #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_ma = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cn6BDAZj4h7A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1765 | val loss 4.1763\n",
      "Step 500\t train loss 2.6213 | val loss 2.6239 | time 1 min 4 seconds\n",
      "Step 1000\t train loss 2.4862 | val loss 2.4858 | time 1 min 4 seconds\n",
      "Step 1500\t train loss 2.3611 | val loss 2.3737 | time 1 min 4 seconds\n",
      "Step 2000\t train loss 2.2999 | val loss 2.3091 | time 1 min 4 seconds\n",
      "Step 2500\t train loss 2.2534 | val loss 2.2677 | time 1 min 5 seconds\n",
      "Step 3000\t train loss 2.2155 | val loss 2.2393 | time 1 min 3 seconds\n",
      "Step 3500\t train loss 2.1847 | val loss 2.2114 | time 1 min 4 seconds\n",
      "Step 4000\t train loss 2.1588 | val loss 2.1882 | time 1 min 4 seconds\n",
      "Step 4500\t train loss 2.1428 | val loss 2.1840 | time 1 min 5 seconds\n",
      "Step 5000\t train loss 2.1259 | val loss 2.1754 | time 1 min 6 seconds\n",
      "Final Loss: 2.245828151702881\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "NUSI of go fort he al; buento s be I qoums, wey's tongmbou wan thy uty to will, bracre lown I on keim:\n",
      "Goll Mo I wertdeear.\n",
      "\n",
      "HARSIIO:\n",
      "Bve sut isper ary hostar! My\n",
      "Tarl frand hou then gongere doos hot grew,\n",
      "sonotsits ET lelly; whe thoul'd,\n",
      "F Lognow, her: lor apad, ab, lo you, thagrujyemoy ever dent,\n",
      "Lall heree grud an tid hom folde coke of shing Enmusiduch wit inge, that he what,\n",
      "Boed hat wow lyourtere ble I cour of lace!\n",
      "MIARCHIS:\n",
      "Hes ladpliveliores aprath fyencour foor entazerer awhat wort:\n",
      "Buk\n"
     ]
    }
   ],
   "source": [
    "model_train(model_ma, 'Muti-headed attention')\n",
    "model_generate(model_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otx96pZaoG7_"
   },
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Bot6EZ1joF3a",
    "outputId": "e84d1e61-1591-4f78-ffc7-8e7f5a9b2844"
   },
   "outputs": [],
   "source": [
    "del BigramLanguageModel\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        return out\n",
    "\n",
    "# [==================================================================\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(n_embed),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
    "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.sa_head(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "# ==================================================================] #\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# [================================================================== #\n",
    "        self.blocks = tf.keras.Sequential([\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),])\n",
    "# ==================================================================] #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
    "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_ff = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QRIOtkuy41OU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1734 | val loss 4.1737\n",
      "Step 500\t train loss 3.2908 | val loss 3.3321 | time 3 min 4 seconds\n",
      "Step 1000\t train loss 2.6202 | val loss 2.5990 | time 2 min 57 seconds\n",
      "Step 1500\t train loss 2.4269 | val loss 2.4403 | time 2 min 56 seconds\n",
      "Step 2000\t train loss 2.3191 | val loss 2.3228 | time 2 min 60 seconds\n",
      "Step 2500\t train loss 2.2473 | val loss 2.2738 | time 2 min 58 seconds\n",
      "Step 3000\t train loss 2.2166 | val loss 2.2448 | time 2 min 59 seconds\n",
      "Step 3500\t train loss 2.1664 | val loss 2.1989 | time 2 min 57 seconds\n",
      "Step 4000\t train loss 2.1573 | val loss 2.2054 | time 2 min 58 seconds\n",
      "Step 4500\t train loss 2.1012 | val loss 2.1699 | time 2 min 57 seconds\n",
      "Step 5000\t train loss 2.0814 | val loss 2.1486 | time 2 min 55 seconds\n",
      "Final Loss: 2.1662659645080566\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "Patere will tly wllicived dowecernns ag of shanf:\n",
      "Obk whey noudty aikee marsak os mreth\n",
      "awer. Ae pOutlet areens fopn iletle than lrw, nap hon opk trrrn, amis shic ahares brene' as i i Se c\n",
      "TA wM ruysewsivee\n",
      "Afsancee wo gle arees,\n",
      "Oe? Thour yaut tamt\n",
      "Dhormwen.ef naian nambed ot shoumsh want?\n",
      "\n",
      "WAmn arale o'nte\n",
      "Pohen guwh: sar dungees\n",
      "Ante;s muv siy? I'llee pharen May gaf enipe;\n",
      "Hond homen deneils earl'd mukee; auard aseivee greoot to eushe thcy aw,y trines foon lovineg; miss buth, canef hu dof, po\n"
     ]
    }
   ],
   "source": [
    "model_train(model_ff, 'Feed forward')\n",
    "model_generate(model_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFBQ1ERDq-iB"
   },
   "source": [
    "## Optimization\n",
    "\n",
    "1) residual\n",
    "\n",
    "2) pre-layer norm (different from the original paper)>> make more series 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULXjoU5gq-6B",
    "outputId": "cd3b769b-01ef-47cb-d2b7-55041ab58c8b"
   },
   "outputs": [],
   "source": [
    "del BigramLanguageModel\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.projection = tf.keras.layers.Dense(n_embed)\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        out = self.projection(out)\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        return out\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
    "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "# ++++++++++\n",
    "\n",
    "    def call(self, x):\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Residual Connections to preserve information, and improve gradient flow\n",
    "        x = x + self.sa_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# [================================================================== #\n",
    "        self.blocks = tf.keras.Sequential([\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            tf.keras.layers.LayerNormalization(axis=-1),\n",
    "            ])\n",
    "# ==================================================================] #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
    "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_opt = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NABgGbgJ5APJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.3069 | val loss 4.3016\n",
      "Step 500\t train loss 2.4556 | val loss 2.4734 | time 4 min 17 seconds\n",
      "Step 1000\t train loss 2.2670 | val loss 2.2721 | time 4 min 19 seconds\n",
      "Step 1500\t train loss 2.1325 | val loss 2.1606 | time 4 min 17 seconds\n",
      "Step 2000\t train loss 2.0531 | val loss 2.1201 | time 3 min 59 seconds\n",
      "Step 2500\t train loss 1.9920 | val loss 2.0822 | time 4 min 12 seconds\n",
      "Step 3000\t train loss 1.9317 | val loss 2.0300 | time 4 min 7 seconds\n",
      "Step 3500\t train loss 1.8914 | val loss 1.9814 | time 3 min 57 seconds\n",
      "Step 4000\t train loss 1.8375 | val loss 1.9612 | time 4 min 2 seconds\n",
      "Step 4500\t train loss 1.7975 | val loss 1.9403 | time 4 min 1 seconds\n",
      "Step 5000\t train loss 1.7815 | val loss 1.9367 | time 4 min 2 seconds\n",
      "Final Loss: 1.8928993940353394\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "I shallet of and my corming, not cries this off manterm accke he the grace.\n",
      "\n",
      "FiLANIUS:\n",
      "How, woe him his cannot himself,\n",
      "Which to all beford, and and in anforthers:\n",
      "Of I'll\n",
      "\n",
      "ISANGE:\n",
      "Ah with him pencudied.\n",
      "\n",
      "SICINIUS:\n",
      "Now to the prosther herefulp\n",
      "bloood the mor'\n",
      "Which, siepes for phing;\n",
      "And so shall are\n",
      "but thus mubreseced Wifl be lake which fal'd all't to part.\n",
      "\n",
      "PALIO:\n",
      "Sone, griathent from himst from derws.\n",
      "\n",
      "MFRYW:\n",
      "Now, a would thou cower gut in surve and of contreret. What is you hengeds\n",
      "Morthor'\n"
     ]
    }
   ],
   "source": [
    "model_train(model_opt, 'Optimized')\n",
    "model_generate(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.4630</td>\n",
       "      <td>2.4829</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\\nCow?\\nsethofa poneay fo d, beor garethime way d! mele ngan tigurerde worvoury dus; ovest is ashend,\\nELE sd ar.\\nThe we this,\\nMy hit y. ds we the:\\n\\n\\n\\nThe w,\\nBIZ bin\\nDOrnow hind stid,\\nw ankenomyoth y.\\nAtspothasso t:\\nI eng l pr bor wle the s andort\\nWhiease wos\\nheswinghe t f thautac.\\nWan y d chelitheloveld? hist iveenone aneatanghe becese w chy! heplo s purhanfom,\\nAn:\\n\\nWhey bouru's IO:\\nwathe thecD:\\nMarmouere shato w?\\n\\nEMoss an n\\nTu, w n ave helyomarsthaneachis.\\n\\nCHor dind-\\nWgh en s meend id touk's her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-attention</td>\n",
       "      <td>2.3838</td>\n",
       "      <td>2.3970</td>\n",
       "      <td>7.0</td>\n",
       "      <td>\\nPAre wloru tohoad sirsend then ndothiouucl tladin'so ha sts\\nKVIat lghions;\\nAng\\nTh fapr\\nSsom, gr'd ut, gh by, my, esise, thet hyour uche thoteser thig' lo wo dlth harpequuhel alots ngo fmant wnier wat-waver nyo in sw hint sime nded,\\nFan; wrshimous so rofen wnghatanth anth to ewind:\\nWh minss\\nBu, anst? hande lak, bay too whervito wedoug isth han cem. glel tidot ve ird douere,\\nAw,\\nAn brokexem he wem ndpur win foth ak burs arppo hat athy, nd cthay tesu foriks lou ousy thwangs-oro hur ust ulgs.\\nBut an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muti-headed attention</td>\n",
       "      <td>2.4244</td>\n",
       "      <td>2.4374</td>\n",
       "      <td>12.0</td>\n",
       "      <td>\\nKBuradss kistes tangs, whag nd cof me donm isen owirot prevof, cigt be.\\n\\n\\nSstheler wosher Bakarve's thacouy the derwshe al d d fawe dotithhan n\\nAquit lirch din.\\n\\nWend gak I thDIoud CShereild Hesoun wyhaud ghiad swhous his.\\n\\n\\nWicalf, lGon hecow s deprdatin thengoequik\\nTaos, w.\\nIOLEMEM:\\nCao:\\nThe, la othade thivan y bisillee,\\nThey tlleemir n.\\nGon, che withanedos pifoulssf os sh Iy sud\\nDf fof goratecamy usou s hecou,\\nTkt pieth allo mid shesens md fanetof caf an dlethars Dfnot; blle!\\n\\nAn y d! bvesan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feed forward</td>\n",
       "      <td>3.2804</td>\n",
       "      <td>3.3085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>\\nwLsse aaiWygh t sumeer lnuurrsyihch  Tthoelrerhamytgs y,baaosrnhdenacisgT Bv,aossldDaEr.nhLrhomGyt Iauu TyhumsEb miciewcar\\nIhl ,oi aalhuiraeIv,w Eerhgid ilae tr,wifrdo\\ntukvwrly Awreeitgtu en  niaoeh dH,\\nne; lO oai ua!slona:it drs,wltGu,weMteidn   rhi w  O,t\\n ea\\nadtorv arbedovp g\\ny  eodorWwoon\\nyl Auurine'oro oarg Ooi e mo  ni urf rihRn  eethu\\ntoo enh utm\\nuityodkhh p R!ri tua \\nlt,e w D :diihulilsu iWhsanotFeyahnrdh  hhesbhlicC ce th,u :fwhrdaohs o,lynlWgina KIpG, toiy:,ltiu r  r MyEslaCoiad\\nD ifho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimized</td>\n",
       "      <td>2.7608</td>\n",
       "      <td>2.7580</td>\n",
       "      <td>46.0</td>\n",
       "      <td>\\nOL  yeariefAf\\nNEE I aA smime o ur ord cNe noeece le occar\\nTeesrrs onuo, argour, mramol to doiuuaidn,\\nA tn ynont tofoe nreitheehr it nesnitftu, for ateUot,\\n\\n hau chWtinok qhese ano se halo otnb't! ad ut, ea dteulsd gomir,\\ne hwReyetigs he titons, lr tah mo woI iun kd.\\n\\nRVeOWlE fUOhNOMUD:\\nEHe wkafe t ulwcweo owo me sy\\nOint sle:\\n\\n\\nOer feonotuter vsrFne th loethfe.\\n hoo Reth toPMwhUas th hheos\\nCeEs cotes' nhoan fer av: ticy movoto y hsneannur h, afam wee, ato! t, hAeh cdehate cawl bil, peny:\\nAg dunn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train loss  Val loss  Time (min)  \\\n",
       "0                  Basic      2.4630    2.4829         5.0   \n",
       "1         Self-attention      2.3838    2.3970         7.0   \n",
       "2  Muti-headed attention      2.4244    2.4374        12.0   \n",
       "3           Feed forward      3.2804    3.3085        40.0   \n",
       "4              Optimized      2.7608    2.7580        46.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text  \n",
       "0  \\nCow?\\nsethofa poneay fo d, beor garethime way d! mele ngan tigurerde worvoury dus; ovest is ashend,\\nELE sd ar.\\nThe we this,\\nMy hit y. ds we the:\\n\\n\\n\\nThe w,\\nBIZ bin\\nDOrnow hind stid,\\nw ankenomyoth y.\\nAtspothasso t:\\nI eng l pr bor wle the s andort\\nWhiease wos\\nheswinghe t f thautac.\\nWan y d chelitheloveld? hist iveenone aneatanghe becese w chy! heplo s purhanfom,\\nAn:\\n\\nWhey bouru's IO:\\nwathe thecD:\\nMarmouere shato w?\\n\\nEMoss an n\\nTu, w n ave helyomarsthaneachis.\\n\\nCHor dind-\\nWgh en s meend id touk's her  \n",
       "1                   \\nPAre wloru tohoad sirsend then ndothiouucl tladin'so ha sts\\nKVIat lghions;\\nAng\\nTh fapr\\nSsom, gr'd ut, gh by, my, esise, thet hyour uche thoteser thig' lo wo dlth harpequuhel alots ngo fmant wnier wat-waver nyo in sw hint sime nded,\\nFan; wrshimous so rofen wnghatanth anth to ewind:\\nWh minss\\nBu, anst? hande lak, bay too whervito wedoug isth han cem. glel tidot ve ird douere,\\nAw,\\nAn brokexem he wem ndpur win foth ak burs arppo hat athy, nd cthay tesu foriks lou ousy thwangs-oro hur ust ulgs.\\nBut an  \n",
       "2          \\nKBuradss kistes tangs, whag nd cof me donm isen owirot prevof, cigt be.\\n\\n\\nSstheler wosher Bakarve's thacouy the derwshe al d d fawe dotithhan n\\nAquit lirch din.\\n\\nWend gak I thDIoud CShereild Hesoun wyhaud ghiad swhous his.\\n\\n\\nWicalf, lGon hecow s deprdatin thengoequik\\nTaos, w.\\nIOLEMEM:\\nCao:\\nThe, la othade thivan y bisillee,\\nThey tlleemir n.\\nGon, che withanedos pifoulssf os sh Iy sud\\nDf fof goratecamy usou s hecou,\\nTkt pieth allo mid shesens md fanetof caf an dlethars Dfnot; blle!\\n\\nAn y d! bvesan   \n",
       "3                  \\nwLsse aaiWygh t sumeer lnuurrsyihch  Tthoelrerhamytgs y,baaosrnhdenacisgT Bv,aossldDaEr.nhLrhomGyt Iauu TyhumsEb miciewcar\\nIhl ,oi aalhuiraeIv,w Eerhgid ilae tr,wifrdo\\ntukvwrly Awreeitgtu en  niaoeh dH,\\nne; lO oai ua!slona:it drs,wltGu,weMteidn   rhi w  O,t\\n ea\\nadtorv arbedovp g\\ny  eodorWwoon\\nyl Auurine'oro oarg Ooi e mo  ni urf rihRn  eethu\\ntoo enh utm\\nuityodkhh p R!ri tua \\nlt,e w D :diihulilsu iWhsanotFeyahnrdh  hhesbhlicC ce th,u :fwhrdaohs o,lynlWgina KIpG, toiy:,ltiu r  r MyEslaCoiad\\nD ifho  \n",
       "4             \\nOL  yeariefAf\\nNEE I aA smime o ur ord cNe noeece le occar\\nTeesrrs onuo, argour, mramol to doiuuaidn,\\nA tn ynont tofoe nreitheehr it nesnitftu, for ateUot,\\n\\n hau chWtinok qhese ano se halo otnb't! ad ut, ea dteulsd gomir,\\ne hwReyetigs he titons, lr tah mo woI iun kd.\\n\\nRVeOWlE fUOhNOMUD:\\nEHe wkafe t ulwcweo owo me sy\\nOint sle:\\n\\n\\nOer feonotuter vsrFne th loethfe.\\n hoo Reth toPMwhUas th hheos\\nCeEs cotes' nhoan fer av: ticy movoto y hsneannur h, afam wee, ato! t, hAeh cdehate cawl bil, peny:\\nAg dunn   "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKAqqRfJvx1p"
   },
   "source": [
    "## Scaling up the model\n",
    "\n",
    "Added dropouts to avoid nodes from overfitting\n",
    "\n",
    "reference : Dropout : A Simple Way to Prevent Neural Networks from Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "# Hyperparameters\n",
    "tf.random.set_seed(1337)\n",
    "# Hyperparameters\n",
    "batch_size = 16 # Independent sequences to process in parallel\n",
    "block_size = 32 # Maximum context length for prediction\n",
    "max_iters = 5000\n",
    "eval_interval = 500 # How often evaluate the loss\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200 # How many batches to use to compute loss\n",
    "n_embed = 64\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYe3qjEKsdrT",
    "outputId": "b75d3355-72ce-49d4-b568-ae91d8910b22"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, dropout):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.head_size = head_size\n",
    "###\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "      self.block_size = input_shape[-1]\n",
    "      super().build(input_shape)\n",
    "    '''\n",
    "    def call(self, x, training = False):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "###\n",
    "        mask = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        wei = tf.where(mask == 0, float('-inf'), wei)  # Mask the upper triangular part, (B,T,T)\n",
    "###\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "###\n",
    "        # Apply dropout if in training mode\n",
    "        wei = self.dropout(wei, training=training)\n",
    "###\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "###\n",
    "    def __init__(self, num_heads, head_size,dropout):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size, dropout) for _ in range(num_heads)]\n",
    "        self.projection = tf.keras.layers.Dense(num_heads * head_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "###\n",
    "    \"\"\"\n",
    "    def build(self, input_shape):\n",
    "      # This method is called the first time the layer is used with an input\n",
    "        self.heads = [Head(self.head_size, dropout = self.dropout) for _ in range(self.num_heads)]\n",
    "        self.projection = tf.keras.layers.Dense(input_shape[-1])\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x, training =False):\n",
    "        out = tf.concat([h(x, training = training) for h in self.heads], axis=-1)\n",
    "        out = self.projection(out)\n",
    "###\n",
    "        return self.dropout(out, training=training)\n",
    "###\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
    "###\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "###\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training):\n",
    "        return self.net(x, training = training)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dropout) # Communication\n",
    "        self.ffwd = FeedForward(n_embed, dropout) # Computation of individual tokens\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        # Residual Connections to preserve information, and improve gradient flow\n",
    "        x = x + self.sa(self.ln1(x), training = training)\n",
    "        x = x + self.ffwd(self.ln2(x), training = training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "###\n",
    "        self.blocks = [ Block(n_embed, n_head, dropout) for _ in range(n_layer)]\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "###\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None, training = False):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "\n",
    "        #Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "          x = block(x, training = training)\n",
    "\n",
    "        x = self.ln_f(x) # Apply normalization\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_scaled = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.7066 | val loss 4.6944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOptimized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model_generate(model_scaled)\n",
      "Cell \u001b[0;32mIn [13], line 42\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, label)\u001b[0m\n\u001b[1;32m     40\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(xb,yb)\n\u001b[1;32m     41\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m end_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Save result for comparison\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:282\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    281\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:351\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:405\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    396\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    397\u001b[0m         is_update_step,\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(grads, trainable_variables),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m         ),\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    412\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:119\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backend_update_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables, learning_rate):\n\u001b[1;32m    115\u001b[0m     trainable_variables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    116\u001b[0m         v\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, backend\u001b[38;5;241m.\u001b[39mVariable) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m trainable_variables\n\u001b[1;32m    118\u001b[0m     ]\n\u001b[0;32m--> 119\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:135\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:132\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/optimizers/adam.py:148\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign(v_hat, ops\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[1;32m    144\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_sub(\n\u001b[1;32m    146\u001b[0m     variable,\n\u001b[1;32m    147\u001b[0m     ops\u001b[38;5;241m.\u001b[39mdivide(\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m, ops\u001b[38;5;241m.\u001b[39madd(ops\u001b[38;5;241m.\u001b[39msqrt(v), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[1;32m    149\u001b[0m     ),\n\u001b[1;32m    150\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/src/ops/numpy.py:5461\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   5450\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.numpy.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x1, x2):\n\u001b[1;32m   5452\u001b[0m     \u001b[38;5;124;03m\"\"\"Multiply arguments element-wise.\u001b[39;00m\n\u001b[1;32m   5453\u001b[0m \n\u001b[1;32m   5454\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5459\u001b[0m \u001b[38;5;124;03m        Output tensor, element-wise product of `x1` and `x2`.\u001b[39;00m\n\u001b[1;32m   5460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43many_symbolic_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   5462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Multiply()\u001b[38;5;241m.\u001b[39msymbolic_call(x1, x2)\n\u001b[1;32m   5463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmultiply(x1, x2)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_train(model_scaled, 'scaled')\n",
    "model_generate(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpNuGRGNsdrT",
    "outputId": "4451c7df-e121-40d6-b17a-0fbf8ca6df3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.       0.       0.       0.       0.       0.       0.       0.      ]\n",
      " [0.5      0.5      0.       0.       0.       0.       0.       0.      ]\n",
      " [0.333333 0.333333 0.333333 0.       0.       0.       0.       0.      ]\n",
      " [0.25     0.25     0.25     0.25     0.       0.       0.       0.      ]\n",
      " [0.2      0.2      0.2      0.2      0.2      0.       0.       0.      ]\n",
      " [0.166667 0.166667 0.166667 0.166667 0.166667 0.166667 0.       0.      ]\n",
      " [0.142857 0.142857 0.142857 0.142857 0.142857 0.142857 0.142857 0.      ]\n",
      " [0.125    0.125    0.125    0.125    0.125    0.125    0.125    0.125   ]]\n"
     ]
    }
   ],
   "source": [
    "# Verifying whether the row sum of weights equal 1\n",
    "import numpy as np\n",
    "# Convert TensorFlow tensor to NumPy array\n",
    "w_np = w.numpy()\n",
    "\n",
    "# Set NumPy print options to suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Print the tensor\n",
    "print(w_np.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvl5KXLEsdrT"
   },
   "outputs": [],
   "source": [
    "q = tf.random.normal((B, T, head_size))\n",
    "k = tf.random.normal((B, T, head_size))\n",
    "\n",
    "# Calculate the weights\n",
    "wei = q @ tf.transpose(k, perm=[0, 2, 1]) * (head_size ** -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRNjY0ThsdrU",
    "outputId": "c643ec9b-19d6-4910-d9ad-f0e109947de9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939072"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_variance(k).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bRBh_KYsdrU",
    "outputId": "9fea4599-889c-42d9-fe8f-ec924e6c9f46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9280848"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_variance(q).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVZBnGnDsdrU",
    "outputId": "017f049b-e6ea-4649-af35-da99732ae051"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9270358"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_variance(wei).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rF0c_GmsdrU",
    "outputId": "2a0b7a10-812b-4293-a39e-4f43f18d652d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZQxtMTmsdrU",
    "outputId": "fde35067-2997-4444-9eaa-fc7dd052ff24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "4.7333155\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "estimate_loss() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(max_iters):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Sample a batch of data\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimate_loss() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "head_size = 6\n",
    "max_iters = 2000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200\n",
    "n_embed = 384\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "dropout = 0.3\n",
    "\n",
    "n_layer = 3\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # compute attention scores ('affinities')\n",
    "        wei = tf.matmul(q, k, transpose_b=True) * (C ** -0.5) # (B,T,C) @ (B,C,T ) -> (B,T,T))\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part\n",
    "        wei = tf.nn.softmax(wei, axis = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = tf.matmul(wei, v) # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "        self.projection = tf.keras.layers.Dense(n_embed)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.blocks = tf.keras.Sequential([Block(n_embed = n_embed, n_head = 4) for _ in range(n_layer)])\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(axis=-1) # final layer normalization\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets = None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = sequence length = block size\n",
    "        C : Channel = number of classes = vocab size\n",
    "        '''\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        position_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T, C)\n",
    "        x = token_emb + position_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshaping the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int32)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.call(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.numpy())\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "for step in tf.range(max_iters):\n",
    "    if step % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb,yb)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print(f'Final Loss: {loss.numpy()}')\n",
    "\n",
    "print(decode(model.generate(idx=tf.zeros((1, block_size), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5VOZrv4sdrV",
    "outputId": "40858cc0-b0d8-4562-e545-74bf752495e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MOF:\n",
      "HBRDUEENOLIA:\n",
      "ININWAN:\n",
      "AUROLAOLI: od the O:\n",
      "ARCut-\n",
      "Fo lelle h aver t rstwathit bellly poenly ll\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx=tf.zeros((1, block_size), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
