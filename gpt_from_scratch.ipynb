{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0   354k      0  0:00:03  0:00:03 --:--:--  354k\n"
     ]
    }
   ],
   "source": [
    "# Downloading tinyshakesphere for training\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspecting the tinyshakespeare text file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the text file\n",
    "with open('tinyshakespeare.txt','r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1115394 characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(text)} characters in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 1000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters (including white space): 65\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Identifying the number of unique characters contained in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters (including white space): {vocab_size}{''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic mapping between characters to integers\n",
    "\n",
    "More sophisticated examples include Google's SentencePiece and OpenAI's tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 52, 1, 42, 47, 45, 47, 58, 57]\n",
      "Shakespeare in digits\n"
     ]
    }
   ],
   "source": [
    "# Assigning numbers to each characters to encode the characters to integers\n",
    "ctoi = {char : num for num, char in enumerate(chars)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "print(encode('Shakespeare in digits'))\n",
    "\n",
    "# Reversely, decode integers back to characters\n",
    "itoc = {num : char for num, char in enumerate(chars)}\n",
    "decode = lambda l : ''.join([itoc[i] for i in l])\n",
    "print(decode(encode('Shakespeare in digits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:43:18.890580: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 15:43:19.162318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 15:43:21.195024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the total text. Adapted the code to work with Tensorflow instead of pytorch\n",
    "import tensorflow as tf\n",
    "data = tf.convert_to_tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data : 1003854\n",
      "Length of test data : 111540\n"
     ]
    }
   ],
   "source": [
    "# Train and validation split sets, with 9:1 ratio\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "print(f'Length of train data : {len(data_train)}\\nLength of test data : {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int32)\n",
      "Input : [18], Output : 47\n",
      "Input : [18 47], Output : 56\n",
      "Input : [18 47 56], Output : 57\n",
      "Input : [18 47 56 57], Output : 58\n",
      "Input : [18 47 56 57 58], Output : 1\n",
      "Input : [18 47 56 57 58  1], Output : 15\n",
      "Input : [18 47 56 57 58  1 15], Output : 47\n",
      "Input : [18 47 56 57 58  1 15 47], Output : 58\n"
     ]
    }
   ],
   "source": [
    "# Starting with block_size implementation\n",
    "block_size = 8\n",
    "print(data_train[:block_size + 1])\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Input : {context}, Output : {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1 51 63  1 41 53 39 58]\n",
      " [39 42  0 20 47 57  1 52]\n",
      " [32 53  1 56 43 60 43 50]\n",
      " [54 39 52 63  1 54 47 43]], shape=(4, 8), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[51 63  1 41 53 39 58  6]\n",
      " [42  0 20 47 57  1 52 39]\n",
      " [53  1 56 43 60 43 50  1]\n",
      " [39 52 63  1 54 47 43 41]], shape=(4, 8), dtype=int32)\n",
      "When input is [1] the target is 51\n",
      "When input is [1, 51] the target is 63\n",
      "When input is [1, 51, 63] the target is 1\n",
      "When input is [1, 51, 63, 1] the target is 41\n",
      "When input is [1, 51, 63, 1, 41] the target is 53\n",
      "When input is [1, 51, 63, 1, 41, 53] the target is 39\n",
      "When input is [1, 51, 63, 1, 41, 53, 39] the target is 58\n",
      "When input is [1, 51, 63, 1, 41, 53, 39, 58] the target is 6\n",
      "When input is [39] the target is 42\n",
      "When input is [39, 42] the target is 0\n",
      "When input is [39, 42, 0] the target is 20\n",
      "When input is [39, 42, 0, 20] the target is 47\n",
      "When input is [39, 42, 0, 20, 47] the target is 57\n",
      "When input is [39, 42, 0, 20, 47, 57] the target is 1\n",
      "When input is [39, 42, 0, 20, 47, 57, 1] the target is 52\n",
      "When input is [39, 42, 0, 20, 47, 57, 1, 52] the target is 39\n",
      "When input is [32] the target is 53\n",
      "When input is [32, 53] the target is 1\n",
      "When input is [32, 53, 1] the target is 56\n",
      "When input is [32, 53, 1, 56] the target is 43\n",
      "When input is [32, 53, 1, 56, 43] the target is 60\n",
      "When input is [32, 53, 1, 56, 43, 60] the target is 43\n",
      "When input is [32, 53, 1, 56, 43, 60, 43] the target is 50\n",
      "When input is [32, 53, 1, 56, 43, 60, 43, 50] the target is 1\n",
      "When input is [54] the target is 39\n",
      "When input is [54, 39] the target is 52\n",
      "When input is [54, 39, 52] the target is 63\n",
      "When input is [54, 39, 52, 63] the target is 1\n",
      "When input is [54, 39, 52, 63, 1] the target is 54\n",
      "When input is [54, 39, 52, 63, 1, 54] the target is 47\n",
      "When input is [54, 39, 52, 63, 1, 54, 47] the target is 43\n",
      "When input is [54, 39, 52, 63, 1, 54, 47, 43] the target is 41\n"
     ]
    }
   ],
   "source": [
    "## To be worked on : packaging the code with script with variables for later\n",
    "# Depiction of the chunk(or in here, block)-wise transformation.\n",
    "# Having varied blocksize allows the algorithm to take into account the context for inference purpose\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(1337) # To be sure to have consistent random number\n",
    "batch_size = 4 # The number of independent sequences to train in parallel\n",
    "block_size = 8 # The maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    # Retrieving batches randomly\n",
    "    ix = tf.random.uniform(shape = (batch_size,),\n",
    "                           maxval = len(data) - block_size,\n",
    "                           dtype = tf.int32)\n",
    "    # Stacking the list of tensors\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb)\n",
    "print(yb)\n",
    "for batch in range(batch_size):\n",
    "    for block in range(block_size):\n",
    "        context = xb[batch, :block+1]\n",
    "        target = yb[batch, block]\n",
    "        print(f'When input is {context.numpy().tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "# A function to average up the loss in multiple batches for both splits\n",
    "# @tf.function : removing the code, despite slower performance as it causes an error\n",
    "def estimate_loss():\n",
    "    output = {}\n",
    "    model.trainable = False # Setting the model to evaluation phase\n",
    "    for split in ['train','val']:\n",
    "        losses = []\n",
    "        for _ in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model.call(X,Y)\n",
    "            losses.append(loss)\n",
    "        output[split] = tf.reduce_mean(losses)\n",
    "    model.trainable = True # Setting the model back to training phase\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "4.175321\n",
      "\n",
      "saUf-Xz-K-?hNk?Yr:r'KUFLHH:QlLboClI\n",
      "oYwnqePrE\n",
      "!zgz'U:,?ZgzxEjItgpzAQjGjM&vv.;OCdqFlQ qxcwcexWhPKs:$&\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = sequence length = block size\n",
    "        C : Channel = number of classes = vocab size\n",
    "        '''\n",
    "        logits = self.token_embedding_table(idx)  # Replacing embedding to the indices\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshaping the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64)\n",
    "\n",
    "#            idx_next = tf.random.categorical(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model.call(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.numpy())\n",
    "\n",
    "print(decode(model.generate(idx=tf.zeros((1, 1), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss 4.1782, val loss 4.1779\n",
      "Step 200: train loss 3.7256, val loss 3.7290\n",
      "Step 400: train loss 3.4030, val loss 3.4141\n",
      "Step 600: train loss 3.1776, val loss 3.1879\n",
      "Step 800: train loss 3.0134, val loss 3.0274\n",
      "Step 1000: train loss 2.8877, val loss 2.9029\n",
      "Step 1200: train loss 2.8012, val loss 2.8117\n",
      "Step 1400: train loss 2.7384, val loss 2.7472\n",
      "Step 1600: train loss 2.6701, val loss 2.6903\n",
      "Step 1800: train loss 2.6506, val loss 2.6526\n",
      "Step 2000: train loss 2.6072, val loss 2.6198\n",
      "Step 2200: train loss 2.5897, val loss 2.6058\n",
      "Step 2400: train loss 2.5700, val loss 2.5763\n",
      "Step 2600: train loss 2.5508, val loss 2.5639\n",
      "Step 2800: train loss 2.5498, val loss 2.5519\n",
      "Step 3000: train loss 2.5291, val loss 2.5377\n",
      "Step 3200: train loss 2.5275, val loss 2.5423\n",
      "Step 3400: train loss 2.5146, val loss 2.5201\n",
      "Step 3600: train loss 2.4991, val loss 2.5187\n",
      "Step 3800: train loss 2.4988, val loss 2.5147\n",
      "Step 4000: train loss 2.4867, val loss 2.5115\n",
      "Step 4200: train loss 2.4924, val loss 2.5031\n",
      "Step 4400: train loss 2.4943, val loss 2.5141\n",
      "Step 4600: train loss 2.4943, val loss 2.5081\n",
      "Step 4800: train loss 2.4781, val loss 2.4942\n",
      "Final Loss: 2.3835971355438232\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for step in tf.range(4000):\n",
    "    if step % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, loss = model(xb,yb)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "print(f'Final Loss: {loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I this issthomfisever ar inkend t jourwhigs w\n",
      "Ang I puithin l anoft were.\n",
      "ADILOrd; w aves y mer I I at, hit, re hthainon'nof S:\n",
      "\n",
      "Vel t fatr oclonc.\n",
      "\n",
      "\n",
      "Why ayo tsind' oyO:\n",
      "RUn, t sth;ORAdear t'strince hot,\n",
      "WAncke!ny, gr'dd bond,\n",
      "PO:\n",
      "Whe m, s be l bth! s wie: nd thimal, ld dero\n",
      "vKA and b s\n",
      "Prtholoncct t m Y$CHeaco hfrd h.\n",
      "D th tomy is peatertur kent ssin; whot w'Thoouedse mythet? h.\n",
      "BOMy heathad t o chare:\n",
      "\n",
      "Antowirdo ESI t\n",
      "\n",
      "QUCKENThecke ce awad tou n amyo gbe be chl:\n",
      "Hid!pe ty be hequ,\n",
      "NAn ts RY dowimat e im fout be br'd lit'l:\n",
      "\n",
      "The inest h ant wimen boruno ir w sisthe thergey h?\n",
      "LAy, p sid hetrt?\n",
      "Thee.\n",
      "NGI hathavesewesterowenthe whiswan l sos th:\n",
      "puthekind uds, for tht leand t mokeabar t blanoe!eatll'Sy\n",
      "A3;-k.\n",
      "Whasorees amesowngrpamis whais!\n",
      "\n",
      "mat aker gend a\n",
      "Foyome aukyolit, fofr'sxpan:\n",
      "Sel f llmbllie utheay\n",
      "Be'd thageabl r yo iof w b\n",
      "SThentor acartondind d; prelog?\n",
      "I th ad d; t o byrcou, y wor ingay! Lo trindicketrouswind.\n",
      "\n",
      "Buprdathindextho y halloure.\n",
      "SCIIVOF$Cr foushe f t se thir,Zu d\n"
     ]
    }
   ],
   "source": [
    "# Generate a sequence\n",
    "idx = tf.zeros((1, 1), dtype=tf.int32)\n",
    "generated_sequence = model.generate(idx, max_new_tokens=1000).numpy()\n",
    "print(decode(generated_sequence[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (T):\n\u001b[1;32m      8\u001b[0m     xprev \u001b[38;5;241m=\u001b[39m x[b, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Batch, including the tth token\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mxbow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(xprev, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    " # Tokens learning from previous context, calculating average of all to previous tokens\n",
    "B,T,C = 4,8,2\n",
    "x = tf.random.uniform(shape=(B, T,C))\n",
    "\n",
    "xbow = tf.zeros((B,T,C))# Defining a bag of words\n",
    "for b in range (B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b, :t+1] # Batch, including the tth token\n",
    "        xbow[b,t] = tf.reduce_mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
