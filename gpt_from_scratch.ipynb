{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmBdax23tCen"
   },
   "source": [
    "### * While the original source code is written in Pytorch, the below code is adapted to Tensorflow.\n",
    "\n",
    "- GPU utilization not enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7ep-bhusdrP"
   },
   "source": [
    "# 1. Preparing the tinyshakespeare text file for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73HKWQbFsdrO",
    "outputId": "34a30494-0117-41ad-fbbd-1e406f800916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  3571k      0 --:--:-- --:--:-- --:--:-- 3583k\n"
     ]
    }
   ],
   "source": [
    "# Downloading tinyshakesphere for training\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > tinyshakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-iatUv1sdrQ",
    "outputId": "e5589b95-a8ec-410e-8568-aae11607bc6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1115394 characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the text file\n",
    "with open('tinyshakespeare.txt','r') as file:\n",
    "    text = file.read()\n",
    "print(f'There are {len(text)} characters in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpDFL78AsdrR",
    "outputId": "8248245e-4a32-437c-ffa4-2000db4e1d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Printing the first 1000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOQjexVqsdrR",
    "outputId": "40af24c8-49a4-4827-cc83-8474e8d485b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters (including white space): 65\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Identifying the number of unique characters contained in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters (including white space): {vocab_size}{''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55wnbME6sdrR"
   },
   "source": [
    "# 2. Basic mapping between characters to integers\n",
    "\n",
    "Tokenizing at the character-level.\n",
    "\n",
    "More sophisticated examples of word encoding include Google's SentencePiece and OpenAI's tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq5eWRmEsdrR",
    "outputId": "9c3f50cf-39b1-4dc7-92aa-c1b14b98d46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 52, 1, 42, 47, 45, 47, 58, 57]\n",
      "Shakespeare in digits\n"
     ]
    }
   ],
   "source": [
    "# Assigning numbers to each characters to encode the characters to integers\n",
    "ctoi = {char : num for num, char in enumerate(chars)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "print(encode('Shakespeare in digits'))\n",
    "\n",
    "# Reversely, decode integers back to characters\n",
    "itoc = {num : char for num, char in enumerate(chars)}\n",
    "decode = lambda l : ''.join([itoc[i] for i in l])\n",
    "print(decode(encode('Shakespeare in digits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6HH69LpsdrR",
    "outputId": "0ebe1d05-56f7-4e6a-9219-cbab913e5836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 18:09:29.470450: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 18:09:29.514064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 18:09:30.482617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115394,) <dtype: 'int32'>\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the total text\n",
    "import tensorflow as tf\n",
    "data = tf.convert_to_tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XHVnVZXsdrR",
    "outputId": "10eee96f-f43d-4a51-ad77-3bba9643db65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data : 1003854\n",
      "Length of test data : 111540\n"
     ]
    }
   ],
   "source": [
    "# Train and validation split sets, with 9:1 ratio\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "print(f'Length of train data : {len(data_train)}\\nLength of test data : {len(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT2C2TlIsdrS",
    "outputId": "9eb40612-cdbe-4c4c-ace2-d4693c8253eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18 47 56 57 58  1 15 47 58], shape=(9,), dtype=int32)\n",
      "Input : [18], Output : 47\n",
      "Input : [18 47], Output : 56\n",
      "Input : [18 47 56], Output : 57\n",
      "Input : [18 47 56 57], Output : 58\n",
      "Input : [18 47 56 57 58], Output : 1\n",
      "Input : [18 47 56 57 58  1], Output : 15\n",
      "Input : [18 47 56 57 58  1 15], Output : 47\n",
      "Input : [18 47 56 57 58  1 15 47], Output : 58\n"
     ]
    }
   ],
   "source": [
    "# Starting with block_size implementation\n",
    "block_size = 8                            # Context length\n",
    "print(data_train[:block_size + 1])\n",
    "x = data_train[:block_size]               # Initial block-size\n",
    "y = data_train[1:block_size+1]            # Next block-size\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Input : {context}, Output : {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpT9xJShsdrS",
    "outputId": "b8e2f7be-2248-4a4b-ccea-caaa90044aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 1 51 63  1 41 53 39 58]\n",
      " [39 42  0 20 47 57  1 52]\n",
      " [32 53  1 56 43 60 43 50]\n",
      " [54 39 52 63  1 54 47 43]], shape=(4, 8), dtype=int32)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[51 63  1 41 53 39 58  6]\n",
      " [42  0 20 47 57  1 52 39]\n",
      " [53  1 56 43 60 43 50  1]\n",
      " [39 52 63  1 54 47 43 41]], shape=(4, 8), dtype=int32)\n",
      "When input is [1] the target is 51\n",
      "When input is [1, 51] the target is 63\n",
      "When input is [1, 51, 63] the target is 1\n",
      "When input is [1, 51, 63, 1] the target is 41\n",
      "When input is [1, 51, 63, 1, 41] the target is 53\n",
      "When input is [1, 51, 63, 1, 41, 53] the target is 39\n",
      "When input is [1, 51, 63, 1, 41, 53, 39] the target is 58\n",
      "When input is [1, 51, 63, 1, 41, 53, 39, 58] the target is 6\n",
      "When input is [39] the target is 42\n",
      "When input is [39, 42] the target is 0\n",
      "When input is [39, 42, 0] the target is 20\n",
      "When input is [39, 42, 0, 20] the target is 47\n",
      "When input is [39, 42, 0, 20, 47] the target is 57\n",
      "When input is [39, 42, 0, 20, 47, 57] the target is 1\n",
      "When input is [39, 42, 0, 20, 47, 57, 1] the target is 52\n",
      "When input is [39, 42, 0, 20, 47, 57, 1, 52] the target is 39\n",
      "When input is [32] the target is 53\n",
      "When input is [32, 53] the target is 1\n",
      "When input is [32, 53, 1] the target is 56\n",
      "When input is [32, 53, 1, 56] the target is 43\n",
      "When input is [32, 53, 1, 56, 43] the target is 60\n",
      "When input is [32, 53, 1, 56, 43, 60] the target is 43\n",
      "When input is [32, 53, 1, 56, 43, 60, 43] the target is 50\n",
      "When input is [32, 53, 1, 56, 43, 60, 43, 50] the target is 1\n",
      "When input is [54] the target is 39\n",
      "When input is [54, 39] the target is 52\n",
      "When input is [54, 39, 52] the target is 63\n",
      "When input is [54, 39, 52, 63] the target is 1\n",
      "When input is [54, 39, 52, 63, 1] the target is 54\n",
      "When input is [54, 39, 52, 63, 1, 54] the target is 47\n",
      "When input is [54, 39, 52, 63, 1, 54, 47] the target is 43\n",
      "When input is [54, 39, 52, 63, 1, 54, 47, 43] the target is 41\n"
     ]
    }
   ],
   "source": [
    "## To be worked on : packaging the code with script with variables for later\n",
    "# Depiction of the chunk(or in here, block)-wise transformation.\n",
    "# Having varied blocksize allows the algorithm to take into account the context for inference purpose\n",
    "\n",
    "tf.random.set_seed(1337) # For reproducibility, to be sure to have consistent random number\n",
    "batch_size = 4 # The number of independent sequences to train in parallel\n",
    "block_size = 8 # The maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    '''\n",
    "    Function to generate a small batch of data of inputs x and targets y\n",
    "    '''\n",
    "\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    # Retrieving batches randomly\n",
    "    ix = tf.random.uniform(shape = (batch_size,),\n",
    "                          maxval = len(data) - block_size,\n",
    "                          dtype = tf.int32)\n",
    "    # Stacking the list of tensors\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "for batch in range(batch_size):       # Batch dimension\n",
    "    for block in range(block_size):   # Time dimension\n",
    "        context = xb[batch, :block+1]\n",
    "        target = yb[batch, block]\n",
    "        print(f'When input is {context.numpy().tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o2p6veM6AH1"
   },
   "source": [
    "## Basic BigramModel for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dQiHX1ws1FPy"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "# Hyperparameters\n",
    "batch_size = 16 # Independent sequences to process in parallel\n",
    "block_size = 32 # Maximum context length for prediction\n",
    "max_iters = 4000\n",
    "eval_interval = 200 # How often evaluate the loss\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200 # How many batches to use to compute loss\n",
    "n_embed = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "zEw2ERgM6KwM",
    "outputId": "a1e968b0-b417-41ac-b5c3-2724e48c0cfe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Train loss, Val loss, Time (min), Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "gpt_results = pd.DataFrame(columns=['Model', 'Train loss', 'Val loss', 'Time (min)', 'Text'])\n",
    "gpt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5cA7HEusdrS",
    "outputId": "3b8f4587-7569-4015-db0d-3700fdd65235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 65)\n",
      "4.1767507\n",
      "\n",
      "sZUf-Xz-K,?hNk;Yr:r'LUFLHH:QlLbpClI\n",
      "oYwnqeOrE\n",
      "!zgz'U:,?ZhzxEjItgpzAQjGjM&vv.;OBdqFlQ pxcwcexWhPKs:$&\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        logits = self.token_embedding_table(idx)  # Replacing embedding to the indices\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_basic = BigramLanguageModel(vocab_size)\n",
    "\n",
    "logits, loss = model_basic.call(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.numpy())\n",
    "\n",
    "print(decode(model_basic.generate(idx=tf.zeros((1, 1), dtype=tf.int32), max_new_tokens=100)[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzC_WFoR4GjM"
   },
   "source": [
    "### Creating an optimizer, and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2GRgIwnCsdrS"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def estimate_loss(model):\n",
    "  '''\n",
    "  Function to average up the loss in multiple batches for both splits\n",
    "  '''\n",
    "  output = {}\n",
    "  model.training = False # Setting the model to evaluation phase\n",
    "  for split in ['train','val']:\n",
    "      losses = []\n",
    "      for _ in range(eval_iters):\n",
    "          X, Y = get_batch(split)\n",
    "          logits, loss = model.call(X,Y)\n",
    "          losses.append(loss)\n",
    "      output[split] = tf.reduce_mean(losses)\n",
    "  model.training = True # Setting the model back to training phase\n",
    "  return output\n",
    "\n",
    "def model_train(model, label):\n",
    "    start_train = time.time()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    for step in tf.range(1, max_iters+1):\n",
    "        if (step % eval_interval == 0) or (step == 1):\n",
    "            losses = estimate_loss(model)\n",
    "            if step != 1:\n",
    "                end_int = time.time()\n",
    "                print(f\"Step {step}\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f} | time {(end_int-start_int)//60:.0f} min {(end_int-start_int)%60:.0f} seconds\")\n",
    "                start_int = time.time()\n",
    "            else:\n",
    "                print(f\"Step {step}\\t\\t train loss {losses['train']:.4f} | val loss {losses['val']:.4f}\")\n",
    "                start_int = time.time()\n",
    "\n",
    "\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "        # Evaluate the loss and update parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, loss = model(xb,yb)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    end_train = time.time()\n",
    "\n",
    "    # Save result for comparison\n",
    "    global gpt_results\n",
    "    gpt_results = pd.concat([gpt_results,pd.DataFrame({'Model': label,\n",
    "                                                       'Train loss': [round(losses['train'].numpy(),4)],\n",
    "                                                       'Val loss': [round(losses['val'].numpy(),4)],\n",
    "                                                       'Time (min)' : [round((end_train-start_train)/60,0)],\n",
    "                                                       'Text':''})], ignore_index = True)\n",
    "    print(f'Final Loss: {loss.numpy()}')\n",
    "\n",
    "def model_generate(model):\n",
    "    # Generate a sequence\n",
    "    print('\\n======================= Generated Sequence =======================')\n",
    "    idx = tf.zeros((1, 1), dtype=tf.int32)\n",
    "    generated_sequence = model.generate(idx, max_new_tokens=500).numpy()\n",
    "    # Save result for comparison\n",
    "    gpt_results.iloc[-1,4] = decode(generated_sequence[0].tolist())\n",
    "    print(decode(generated_sequence[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "ziuASsv3sdrT",
    "outputId": "a5433902-7e34-4cdf-d0de-08f205a2da78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1765 | val loss 4.1764\n",
      "Step 200\t train loss 3.5837 | val loss 3.5889 | time 0 min 13 seconds\n",
      "Step 400\t train loss 3.2158 | val loss 3.2263 | time 0 min 13 seconds\n",
      "Step 600\t train loss 2.9821 | val loss 2.9912 | time 0 min 13 seconds\n",
      "Step 800\t train loss 2.8323 | val loss 2.8417 | time 0 min 13 seconds\n",
      "Step 1000\t train loss 2.7273 | val loss 2.7366 | time 0 min 13 seconds\n",
      "Step 1200\t train loss 2.6691 | val loss 2.6733 | time 0 min 13 seconds\n",
      "Step 1400\t train loss 2.6253 | val loss 2.6309 | time 0 min 13 seconds\n",
      "Step 1600\t train loss 2.5864 | val loss 2.5955 | time 0 min 13 seconds\n",
      "Step 1800\t train loss 2.5622 | val loss 2.5695 | time 0 min 13 seconds\n",
      "Step 2000\t train loss 2.5446 | val loss 2.5544 | time 0 min 13 seconds\n",
      "Step 2200\t train loss 2.5260 | val loss 2.5383 | time 0 min 12 seconds\n",
      "Step 2400\t train loss 2.5186 | val loss 2.5269 | time 0 min 12 seconds\n",
      "Step 2600\t train loss 2.5108 | val loss 2.5247 | time 0 min 12 seconds\n",
      "Step 2800\t train loss 2.5060 | val loss 2.5131 | time 0 min 12 seconds\n",
      "Step 3000\t train loss 2.4997 | val loss 2.5099 | time 0 min 12 seconds\n",
      "Step 3200\t train loss 2.4891 | val loss 2.5005 | time 0 min 12 seconds\n",
      "Step 3400\t train loss 2.4957 | val loss 2.4960 | time 0 min 13 seconds\n",
      "Step 3600\t train loss 2.4777 | val loss 2.4930 | time 0 min 13 seconds\n",
      "Step 3800\t train loss 2.4807 | val loss 2.4901 | time 0 min 13 seconds\n",
      "Step 4000\t train loss 2.4711 | val loss 2.4878 | time 0 min 13 seconds\n",
      "Final Loss: 2.412485122680664\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "D g'sou\n",
      "G mese;\n",
      "We RWAmey stan fogas Gis w,\n",
      "Tand,parit amae ghe!nd acoracore.\n",
      "S:\n",
      "AY s hend y VI ll t t ace ben;\n",
      "Whee t thapoush, tes flyogen qhe\n",
      "Thare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\n",
      "\n",
      "\n",
      "\n",
      "Whe n.\n",
      "\n",
      "RDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth \n"
     ]
    }
   ],
   "source": [
    "model_train(model_basic, 'Basic')\n",
    "model_generate(model_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.4711</td>\n",
       "      <td>2.4878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>\\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model  Train loss  Val loss  Time (min)  \\\n",
       "0  Basic      2.4711    2.4878         4.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Text  \n",
       "0  \\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth   "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt-q5K1BsdrT"
   },
   "source": [
    "## The mathematical trick in self-attention\n",
    "Below present different ways of calculating weighted aggregation of a matrix, from beginning of the block in each batch, up to the 't'th token. The results of the four approaches are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "p1Ypwptzhc6a"
   },
   "outputs": [],
   "source": [
    "# Tokens learning from previous context, by calculating average up to 't'th token\n",
    "B,T,C = 4,8,2 # Batch, Time, Channels\n",
    "x = tf.random.uniform(shape=(B, T,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iqndYlwH3kp"
   },
   "source": [
    "### Version 1: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-eOjLoiqsdrT"
   },
   "outputs": [],
   "source": [
    "xbow = tf.zeros((B,T,C)) # Defining a bag of words\n",
    "for b in range (B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b, :t+1] # (t, C) Batch, including the 't'th token\n",
    "        xbow = xbow.numpy()  # Convert xbow to numpy array to support assignment\n",
    "        xbow[b, t] = tf.reduce_mean(xprev, axis=0).numpy()  # Calculate mean and assign to xbow\n",
    "        xbow = tf.convert_to_tensor(xbow)  # Convert back to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vamUFUHssdrT"
   },
   "source": [
    "### Version 2: Vectorizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwFOca6CsdrT",
    "outputId": "792914a6-e576-4fb4-c2f9-0f034d7fb97c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = w / tf.math.reduce_sum(w, axis = 1, keepdims = True) # Low triangular matrix for calculating average weights\n",
    "\n",
    "xbow2 = w @ x # (B, T, T) @ (B , T, C) --> (B, T, C)\n",
    "tf.experimental.numpy.allclose(xbow,xbow2).numpy() # Checking whether xbow == xbow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auCbs_uBIBei"
   },
   "source": [
    "### Version 3: Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mloGBUAzsdrT",
    "outputId": "63cfa72d-94dc-4adb-9b32-bfbf5b4a89fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = tf.zeros((T,T))\n",
    "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
    "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
    "xbow3 = w @ x\n",
    "tf.experimental.numpy.allclose(xbow,xbow3).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVt_n-VReoxa"
   },
   "source": [
    "### Version 4: Self-attention\n",
    "\n",
    "Called self-attention as the key, query and value are generated from the same value (x)\n",
    "\n",
    "Note that key and query weights values are different as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UtHbUZCerfL",
    "outputId": "83a40638-7be2-45de-b262-9440fb9b5d58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention mechanism\n",
    "head_size = 16\n",
    "key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "k = key(x) # Weights adjusted, (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "w = q @ tf.transpose(k, perm=[0,2,1]) # (B, T, 16) @ (B, 16, T) -> (B, T, T), with (T, T) indicating elements compared with every element in the sequence\n",
    "\n",
    "tril = tf.linalg.band_part(tf.ones((T,T)),num_lower = 8, num_upper= 0)\n",
    "w = tf.where(tril == 0, float('-inf'), w) # Replacing 0s with -inf, indicating that the past blocks cannot communicate with the future blocks\n",
    "w = tf.nn.softmax(w, axis = -1) # Normalizing the weight matrix\n",
    "\n",
    "v = value(x)\n",
    "out = w @ v # Using aggregated value instead of the raw x for dimensionality reduction, information extraction\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgtNLQkCsdrT"
   },
   "source": [
    "Notes:\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over as a set of vectors. This is why we need to positionally encode tokens\n",
    "- Each example across batch dimension is of course processed completely independently and never 'talk' to each other\n",
    "- In an 'encoder' attention block (w = tf.where(tril == 0, float('-inf'), w))code can be omitted, allowing all tokens to communicate. This block here is called a 'decoder' attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- 'Self attention' just means that the keys and values are produced from the same source as queries. In 'Cross-attention', the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- 'Scaled' attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, wei will be unit variance too and softmax will stay diffuses and not saturate too much, Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL_HY8n3sdrT"
   },
   "source": [
    "## Modified BigramModel with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4JRlLv4LFms",
    "outputId": "c468d4a1-3850-4c18-d7fb-9c99adfddf09"
   },
   "outputs": [],
   "source": [
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_sa = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zqlLK9RT4HBq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1742 | val loss 4.1743\n",
      "Step 200\t train loss 3.0306 | val loss 3.0350 | time 0 min 24 seconds\n",
      "Step 400\t train loss 2.6775 | val loss 2.6700 | time 0 min 24 seconds\n",
      "Step 600\t train loss 2.5807 | val loss 2.5806 | time 0 min 24 seconds\n",
      "Step 800\t train loss 2.5430 | val loss 2.5285 | time 0 min 24 seconds\n",
      "Step 1000\t train loss 2.5057 | val loss 2.5074 | time 0 min 24 seconds\n",
      "Step 1200\t train loss 2.4841 | val loss 2.4867 | time 0 min 22 seconds\n",
      "Step 1400\t train loss 2.4767 | val loss 2.4711 | time 0 min 23 seconds\n",
      "Step 1600\t train loss 2.4599 | val loss 2.4606 | time 0 min 23 seconds\n",
      "Step 1800\t train loss 2.4464 | val loss 2.4581 | time 0 min 23 seconds\n",
      "Step 2000\t train loss 2.4312 | val loss 2.4490 | time 0 min 23 seconds\n",
      "Step 2200\t train loss 2.4291 | val loss 2.4370 | time 0 min 23 seconds\n",
      "Step 2400\t train loss 2.4161 | val loss 2.4245 | time 0 min 23 seconds\n",
      "Step 2600\t train loss 2.3946 | val loss 2.4065 | time 0 min 22 seconds\n",
      "Step 2800\t train loss 2.3915 | val loss 2.4095 | time 0 min 22 seconds\n",
      "Step 3000\t train loss 2.3816 | val loss 2.3883 | time 0 min 22 seconds\n",
      "Step 3200\t train loss 2.3642 | val loss 2.3836 | time 0 min 22 seconds\n",
      "Step 3400\t train loss 2.3660 | val loss 2.3784 | time 0 min 21 seconds\n",
      "Step 3600\t train loss 2.3566 | val loss 2.3842 | time 0 min 21 seconds\n",
      "Step 3800\t train loss 2.3553 | val loss 2.3713 | time 0 min 21 seconds\n",
      "Step 4000\t train loss 2.3557 | val loss 2.3745 | time 0 min 21 seconds\n",
      "Final Loss: 2.34033203125\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "Grth berded ourdar to, hey, herirod;, therd yod wino!\n",
      "WARNG:\n",
      "Bes,\n",
      "I fad wis the: isth gears cituy f ou boerilercors yon,\n",
      "Thord thinticer ofo wieche I INorurdea randexalan.\n",
      "Nou say\n",
      "Anel fou byo cy. ARCEO:\n",
      "Lou Ene toe, paer steran forve Pou\n",
      "ies th aried inlon uan thelem pe't\n",
      "sical akitaly, mave,-juegh.\n",
      "\n",
      "An'TO:\n",
      "TAno\n",
      "Whir\n",
      "Laveit'd; thind 'sourd.\n",
      "\n",
      "BELLO ave me he?\n",
      "INou y thels ld\n",
      "Whexy ato dove wimarenas by at\n",
      "Bur. KARTHanit maved,\n",
      "ABRUK:\n",
      "\n",
      "By; osd ansch pert torow'd comicewhe ke!\n",
      "Fak, gythesard tey q\n"
     ]
    }
   ],
   "source": [
    "model_train(model_sa,'Self-attention')\n",
    "model_generate(model_sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqcACneZN9rj"
   },
   "source": [
    "## Multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZNYqeA4sdrT",
    "outputId": "ff965f06-a210-4053-c33e-babf2fba029f"
   },
   "outputs": [],
   "source": [
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "# ================================================================== #\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        return out\n",
    "# ================================================================== #\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "# ================================================================== #\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# ================================================================== #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply one head of self-attention (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_ma = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Cn6BDAZj4h7A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1748 | val loss 4.1747\n",
      "Step 200\t train loss 3.0048 | val loss 3.0125 | time 0 min 35 seconds\n",
      "Step 400\t train loss 2.6610 | val loss 2.6532 | time 0 min 35 seconds\n",
      "Step 600\t train loss 2.5831 | val loss 2.5865 | time 0 min 35 seconds\n",
      "Step 800\t train loss 2.5263 | val loss 2.5184 | time 0 min 34 seconds\n",
      "Step 1000\t train loss 2.4590 | val loss 2.4714 | time 0 min 34 seconds\n",
      "Step 1200\t train loss 2.4220 | val loss 2.4310 | time 0 min 34 seconds\n",
      "Step 1400\t train loss 2.3970 | val loss 2.3991 | time 0 min 36 seconds\n",
      "Step 1600\t train loss 2.3669 | val loss 2.3782 | time 0 min 36 seconds\n",
      "Step 1800\t train loss 2.3467 | val loss 2.3428 | time 0 min 36 seconds\n",
      "Step 2000\t train loss 2.3231 | val loss 2.3349 | time 0 min 35 seconds\n",
      "Step 2200\t train loss 2.3083 | val loss 2.3140 | time 0 min 34 seconds\n",
      "Step 2400\t train loss 2.2925 | val loss 2.2990 | time 0 min 33 seconds\n",
      "Step 2600\t train loss 2.2805 | val loss 2.2778 | time 0 min 32 seconds\n",
      "Step 2800\t train loss 2.2599 | val loss 2.2654 | time 0 min 33 seconds\n",
      "Step 3000\t train loss 2.2401 | val loss 2.2544 | time 0 min 32 seconds\n",
      "Step 3200\t train loss 2.2279 | val loss 2.2553 | time 0 min 32 seconds\n",
      "Step 3400\t train loss 2.2250 | val loss 2.2436 | time 0 min 31 seconds\n",
      "Step 3600\t train loss 2.2050 | val loss 2.2273 | time 0 min 30 seconds\n",
      "Step 3800\t train loss 2.1915 | val loss 2.2302 | time 0 min 30 seconds\n",
      "Step 4000\t train loss 2.1920 | val loss 2.2168 | time 0 min 29 seconds\n",
      "Final Loss: 2.0784831047058105\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "COROProm:\n",
      "I geast mest bl fath my hist ss arysupy\n",
      "An?\n",
      "PEAERCIDA:\n",
      "IN a thembutt,\n",
      "Pay avecse, aswe k's bas ters no grark, klive's.\n",
      "KE WIG irt;\n",
      "Wer\n",
      "Fot lasupave penot you pe bibe! of thy ty me-he; I o be han, arizede yor yougs timpid, band dend the 'd weit my stragn.\n",
      "\n",
      "Fas ma this showio himm by cingee o, for cen's!\n",
      "HARDILO:\n",
      "I dys thered.\n",
      "And coul the 'swse? ay, anent thrid im's\n",
      "D INIUS:\n",
      "And, aglent, Le OF eve o wamledse noumse I yot,'lse wit I ter ball hy leengin bown.\n",
      "\n",
      "Cithine\n",
      "Hen mbe lore bo whew\n"
     ]
    }
   ],
   "source": [
    "model_train(model_ma, 'Muti-headed attention')\n",
    "model_generate(model_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otx96pZaoG7_"
   },
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "Bot6EZ1joF3a",
    "outputId": "e84d1e61-1591-4f78-ffc7-8e7f5a9b2844"
   },
   "outputs": [],
   "source": [
    "del BigramLanguageModel\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "        return out\n",
    "\n",
    "# [==================================================================\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(n_embed),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
    "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.sa_head(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "# ==================================================================] #\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# [================================================================== #\n",
    "        self.blocks = tf.keras.Sequential([\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),])\n",
    "# ==================================================================] #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
    "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_ff = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QRIOtkuy41OU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.1763 | val loss 4.1763\n",
      "Step 200\t train loss 3.3198 | val loss 3.3589 | time 1 min 31 seconds\n",
      "Step 400\t train loss 3.3161 | val loss 3.3515 | time 1 min 30 seconds\n",
      "Step 600\t train loss 3.2893 | val loss 3.3227 | time 1 min 31 seconds\n",
      "Step 800\t train loss 3.0230 | val loss 3.0169 | time 1 min 31 seconds\n",
      "Step 1000\t train loss 2.6803 | val loss 2.6722 | time 1 min 29 seconds\n",
      "Step 1200\t train loss 2.5700 | val loss 2.5629 | time 1 min 31 seconds\n",
      "Step 1400\t train loss 2.4762 | val loss 2.4689 | time 1 min 31 seconds\n",
      "Step 1600\t train loss 2.4362 | val loss 2.4405 | time 1 min 30 seconds\n",
      "Step 1800\t train loss 2.3859 | val loss 2.3728 | time 1 min 32 seconds\n",
      "Step 2000\t train loss 2.3389 | val loss 2.3395 | time 1 min 31 seconds\n",
      "Step 2200\t train loss 2.3320 | val loss 2.3251 | time 1 min 29 seconds\n",
      "Step 2400\t train loss 2.2896 | val loss 2.3039 | time 1 min 32 seconds\n",
      "Step 2600\t train loss 2.2616 | val loss 2.2829 | time 1 min 31 seconds\n",
      "Step 2800\t train loss 2.2305 | val loss 2.2467 | time 1 min 33 seconds\n",
      "Step 3000\t train loss 2.2156 | val loss 2.2416 | time 1 min 31 seconds\n",
      "Step 3200\t train loss 2.1967 | val loss 2.2233 | time 1 min 31 seconds\n",
      "Step 3400\t train loss 2.1679 | val loss 2.2093 | time 1 min 32 seconds\n",
      "Step 3600\t train loss 2.1764 | val loss 2.2106 | time 1 min 30 seconds\n",
      "Step 3800\t train loss 2.1599 | val loss 2.1994 | time 1 min 32 seconds\n",
      "Step 4000\t train loss 2.1342 | val loss 2.1828 | time 1 min 33 seconds\n",
      "Final Loss: 2.1758975982666016\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "To be now as lodn.\n",
      "\n",
      "BENCIHUS:\n",
      "The of the hail'l?\n",
      "Yored ouark taine sins. if to wruse:\n",
      "not bickus the a onay pern thape silone therem.\n",
      "bom the aence,\n",
      "Thath ome ounceing magy thisen.\n",
      "Nard omlat; bas plailne? of oskes!\n",
      "Yourm.r ther olit oun to sepucesfirse lied. baissh huny; goops oupeen, Tiver in nor thee ingthech thouech, I farter er'p\n",
      "lath thee hake: a thath,\n",
      "To you the cise merting astert tilstlevoe: maight, nonmmlys\n",
      "I ound, eoccuz poiedaung\n",
      "Foorl ither's a and townt,\n",
      "Ow ece.srfeer have ageand,\n"
     ]
    }
   ],
   "source": [
    "model_train(model_ff, 'Feed forward')\n",
    "model_generate(model_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFBQ1ERDq-iB"
   },
   "source": [
    "## Optimization\n",
    "\n",
    "1) residual\n",
    "\n",
    "2) pre-layer norm (different from the original paper)>> make more series 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULXjoU5gq-6B",
    "outputId": "cd3b769b-01ef-47cb-d2b7-55041ab58c8b"
   },
   "outputs": [],
   "source": [
    "del BigramLanguageModel\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.tril = tf.constant(tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0), dtype= tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "        wei = tf.where(self.tril[:T, :T] == 0, float('-inf'), wei) # Mask the upper triangular part, (B,T,T)\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "\n",
    "    def __init__(self,num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size) for _ in range(num_heads)]\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.projection = tf.keras.layers.Dense(n_embed)\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    def call(self, x):\n",
    "        out = tf.concat([h(x) for h in self.heads], axis=-1)\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        out = self.projection(out)\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        return out\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(n_head, n_embed//n_head) # Communication\n",
    "        self.ffwd = FeedForward(n_embed) # Computation of individual tokens\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "# ++++++++++\n",
    "\n",
    "    def call(self, x):\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # Residual Connections to preserve information, and improve gradient flow\n",
    "        x = x + self.sa_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4) # 4 heads of 8-dimensional self-attention\n",
    "# [================================================================== #\n",
    "        self.blocks = tf.keras.Sequential([\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            tf.keras.layers.LayerNormalization(axis=-1),\n",
    "            ])\n",
    "# ==================================================================] #\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "        x = self.sa_head(x) # Apply self-attention (B, T, C)\n",
    "        x = self.blocks(x) # Apply feed forward (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_opt = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NABgGbgJ5APJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.5859 | val loss 4.5831\n",
      "Step 200\t train loss 3.1861 | val loss 3.2109 | time 2 min 4 seconds\n",
      "Step 400\t train loss 2.5651 | val loss 2.5561 | time 2 min 0 seconds\n",
      "Step 600\t train loss 2.4196 | val loss 2.4187 | time 2 min 5 seconds\n",
      "Step 800\t train loss 2.3521 | val loss 2.3549 | time 1 min 49 seconds\n",
      "Step 1000\t train loss 2.2909 | val loss 2.2963 | time 1 min 51 seconds\n",
      "Step 1200\t train loss 2.2647 | val loss 2.2715 | time 1 min 50 seconds\n",
      "Step 1400\t train loss 2.2015 | val loss 2.2031 | time 1 min 51 seconds\n",
      "Step 1600\t train loss 2.1612 | val loss 2.1841 | time 1 min 48 seconds\n",
      "Step 1800\t train loss 2.1083 | val loss 2.1551 | time 1 min 49 seconds\n",
      "Step 2000\t train loss 2.0918 | val loss 2.1425 | time 1 min 49 seconds\n",
      "Step 2200\t train loss 2.0470 | val loss 2.0984 | time 1 min 49 seconds\n",
      "Step 2400\t train loss 2.0153 | val loss 2.0836 | time 1 min 48 seconds\n",
      "Step 2600\t train loss 1.9748 | val loss 2.0495 | time 1 min 47 seconds\n",
      "Step 2800\t train loss 1.9626 | val loss 2.0407 | time 1 min 49 seconds\n",
      "Step 3000\t train loss 1.9275 | val loss 2.0266 | time 1 min 46 seconds\n",
      "Step 3200\t train loss 1.9203 | val loss 2.0090 | time 1 min 52 seconds\n",
      "Step 3400\t train loss 1.8968 | val loss 2.0001 | time 1 min 50 seconds\n",
      "Step 3600\t train loss 1.8836 | val loss 1.9876 | time 2 min 2 seconds\n",
      "Step 3800\t train loss 1.8606 | val loss 1.9864 | time 1 min 58 seconds\n",
      "Step 4000\t train loss 1.8519 | val loss 1.9671 | time 1 min 56 seconds\n",
      "Final Loss: 1.9688377380371094\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "The ridpers rrace shall I ackie!\n",
      "Ifty to gate to pese have tread it:\n",
      "A matelliegly lery tcounte uppoter.\n",
      "\n",
      "VOLINIUS:\n",
      "E:\n",
      "Aze hid of a somers deep'lle have for unwordy.\n",
      "Alack them\n",
      "Astin to o as me! way I\n",
      "That 'cles thou with them lears\n",
      "but wate kin harnicbiod, thy laud in we rifh\n",
      "Reced whith say love to manite mode beate.\n",
      "\n",
      "BELID:\n",
      "You my, cast thin the pold, tell dephars. Mire a cuing onclesmlance your,\n",
      "On you hisgt ate to burnuege, trots gontedn; on tesw?\n",
      "Do she amper you' lather of hasir Venrowing\n"
     ]
    }
   ],
   "source": [
    "model_train(model_opt, 'Optimized')\n",
    "model_generate(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.4711</td>\n",
       "      <td>2.4878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>\\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-attention</td>\n",
       "      <td>2.3557</td>\n",
       "      <td>2.3745</td>\n",
       "      <td>8.0</td>\n",
       "      <td>\\nGrth berded ourdar to, hey, herirod;, therd yod wino!\\nWARNG:\\nBes,\\nI fad wis the: isth gears cituy f ou boerilercors yon,\\nThord thinticer ofo wieche I INorurdea randexalan.\\nNou say\\nAnel fou byo cy. ARCEO:\\nLou Ene toe, paer steran forve Pou\\nies th aried inlon uan thelem pe't\\nsical akitaly, mave,-juegh.\\n\\nAn'TO:\\nTAno\\nWhir\\nLaveit'd; thind 'sourd.\\n\\nBELLO ave me he?\\nINou y thels ld\\nWhexy ato dove wimarenas by at\\nBur. KARTHanit maved,\\nABRUK:\\n\\nBy; osd ansch pert torow'd comicewhe ke!\\nFak, gythesard tey q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muti-headed attention</td>\n",
       "      <td>2.1920</td>\n",
       "      <td>2.2168</td>\n",
       "      <td>11.0</td>\n",
       "      <td>\\nCOROProm:\\nI geast mest bl fath my hist ss arysupy\\nAn?\\nPEAERCIDA:\\nIN a thembutt,\\nPay avecse, aswe k's bas ters no grark, klive's.\\nKE WIG irt;\\nWer\\nFot lasupave penot you pe bibe! of thy ty me-he; I o be han, arizede yor yougs timpid, band dend the 'd weit my stragn.\\n\\nFas ma this showio himm by cingee o, for cen's!\\nHARDILO:\\nI dys thered.\\nAnd coul the 'swse? ay, anent thrid im's\\nD INIUS:\\nAnd, aglent, Le OF eve o wamledse noumse I yot,'lse wit I ter ball hy leengin bown.\\n\\nCithine\\nHen mbe lore bo whew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feed forward</td>\n",
       "      <td>2.1342</td>\n",
       "      <td>2.1828</td>\n",
       "      <td>31.0</td>\n",
       "      <td>\\nTo be now as lodn.\\n\\nBENCIHUS:\\nThe of the hail'l?\\nYored ouark taine sins. if to wruse:\\nnot bickus the a onay pern thape silone therem.\\nbom the aence,\\nThath ome ounceing magy thisen.\\nNard omlat; bas plailne? of oskes!\\nYourm.r ther olit oun to sepucesfirse lied. baissh huny; goops oupeen, Tiver in nor thee ingthech thouech, I farter er'p\\nlath thee hake: a thath,\\nTo you the cise merting astert tilstlevoe: maight, nonmmlys\\nI ound, eoccuz poiedaung\\nFoorl ither's a and townt,\\nOw ece.srfeer have ageand,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimized</td>\n",
       "      <td>1.8519</td>\n",
       "      <td>1.9671</td>\n",
       "      <td>38.0</td>\n",
       "      <td>\\nThe ridpers rrace shall I ackie!\\nIfty to gate to pese have tread it:\\nA matelliegly lery tcounte uppoter.\\n\\nVOLINIUS:\\nE:\\nAze hid of a somers deep'lle have for unwordy.\\nAlack them\\nAstin to o as me! way I\\nThat 'cles thou with them lears\\nbut wate kin harnicbiod, thy laud in we rifh\\nReced whith say love to manite mode beate.\\n\\nBELID:\\nYou my, cast thin the pold, tell dephars. Mire a cuing onclesmlance your,\\nOn you hisgt ate to burnuege, trots gontedn; on tesw?\\nDo she amper you' lather of hasir Venrowing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train loss  Val loss  Time (min)  \\\n",
       "0                  Basic      2.4711    2.4878         4.0   \n",
       "1         Self-attention      2.3557    2.3745         8.0   \n",
       "2  Muti-headed attention      2.1920    2.2168        11.0   \n",
       "3           Feed forward      2.1342    2.1828        31.0   \n",
       "4              Optimized      1.8519    1.9671        38.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Text  \n",
       "0            \\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth   \n",
       "1  \\nGrth berded ourdar to, hey, herirod;, therd yod wino!\\nWARNG:\\nBes,\\nI fad wis the: isth gears cituy f ou boerilercors yon,\\nThord thinticer ofo wieche I INorurdea randexalan.\\nNou say\\nAnel fou byo cy. ARCEO:\\nLou Ene toe, paer steran forve Pou\\nies th aried inlon uan thelem pe't\\nsical akitaly, mave,-juegh.\\n\\nAn'TO:\\nTAno\\nWhir\\nLaveit'd; thind 'sourd.\\n\\nBELLO ave me he?\\nINou y thels ld\\nWhexy ato dove wimarenas by at\\nBur. KARTHanit maved,\\nABRUK:\\n\\nBy; osd ansch pert torow'd comicewhe ke!\\nFak, gythesard tey q  \n",
       "2       \\nCOROProm:\\nI geast mest bl fath my hist ss arysupy\\nAn?\\nPEAERCIDA:\\nIN a thembutt,\\nPay avecse, aswe k's bas ters no grark, klive's.\\nKE WIG irt;\\nWer\\nFot lasupave penot you pe bibe! of thy ty me-he; I o be han, arizede yor yougs timpid, band dend the 'd weit my stragn.\\n\\nFas ma this showio himm by cingee o, for cen's!\\nHARDILO:\\nI dys thered.\\nAnd coul the 'swse? ay, anent thrid im's\\nD INIUS:\\nAnd, aglent, Le OF eve o wamledse noumse I yot,'lse wit I ter ball hy leengin bown.\\n\\nCithine\\nHen mbe lore bo whew  \n",
       "3           \\nTo be now as lodn.\\n\\nBENCIHUS:\\nThe of the hail'l?\\nYored ouark taine sins. if to wruse:\\nnot bickus the a onay pern thape silone therem.\\nbom the aence,\\nThath ome ounceing magy thisen.\\nNard omlat; bas plailne? of oskes!\\nYourm.r ther olit oun to sepucesfirse lied. baissh huny; goops oupeen, Tiver in nor thee ingthech thouech, I farter er'p\\nlath thee hake: a thath,\\nTo you the cise merting astert tilstlevoe: maight, nonmmlys\\nI ound, eoccuz poiedaung\\nFoorl ither's a and townt,\\nOw ece.srfeer have ageand,  \n",
       "4         \\nThe ridpers rrace shall I ackie!\\nIfty to gate to pese have tread it:\\nA matelliegly lery tcounte uppoter.\\n\\nVOLINIUS:\\nE:\\nAze hid of a somers deep'lle have for unwordy.\\nAlack them\\nAstin to o as me! way I\\nThat 'cles thou with them lears\\nbut wate kin harnicbiod, thy laud in we rifh\\nReced whith say love to manite mode beate.\\n\\nBELID:\\nYou my, cast thin the pold, tell dephars. Mire a cuing onclesmlance your,\\nOn you hisgt ate to burnuege, trots gontedn; on tesw?\\nDo she amper you' lather of hasir Venrowing  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKAqqRfJvx1p"
   },
   "source": [
    "## Scaling up the model\n",
    "\n",
    "Added dropouts to avoid nodes from overfitting\n",
    "\n",
    "reference : Dropout : A Simple Way to Prevent Neural Networks from Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "# Hyperparameters\n",
    "batch_size = 16 # Independent sequences to process in parallel\n",
    "block_size = 32 # Maximum context length for prediction\n",
    "max_iters = 4000\n",
    "eval_interval = 200 # How often evaluate the loss\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200 # How many batches to use to compute loss\n",
    "n_embed = 64\n",
    "n_head = 2\n",
    "n_layer = 2\n",
    "dropout = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "xYe3qjEKsdrT",
    "outputId": "b75d3355-72ce-49d4-b568-ae91d8910b22"
   },
   "outputs": [],
   "source": [
    "del BigramLanguageModel\n",
    "\n",
    "class Head(tf.keras.Model):\n",
    "    \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, dropout):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(Head, self).__init__()\n",
    "        self.key = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.query = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.value = tf.keras.layers.Dense(units=head_size, use_bias=False)\n",
    "        self.head_size = head_size\n",
    "###\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "      self.block_size = input_shape[-1]\n",
    "      super().build(input_shape)\n",
    "    '''\n",
    "    def call(self, x, training = False):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x)   # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ tf.transpose(k, perm=[0,2,1]) * C ** (-0.5) # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
    "###\n",
    "        mask = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
    "        wei = tf.where(mask == 0, float('-inf'), wei)  # Mask the upper triangular part, (B,T,T)\n",
    "###\n",
    "        wei = tf.nn.softmax(wei, axis = -1) # (B,T,T)\n",
    "###\n",
    "        # Apply dropout if in training mode\n",
    "        wei = self.dropout(wei, training=training)\n",
    "###\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    '''Multiple heads of self-attention in parallel'''\n",
    "###\n",
    "    def __init__(self, num_heads, head_size,dropout):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size, dropout) for _ in range(num_heads)]\n",
    "        self.projection = tf.keras.layers.Dense(num_heads * head_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "###\n",
    "    \"\"\"\n",
    "    def build(self, input_shape):\n",
    "      # This method is called the first time the layer is used with an input\n",
    "        self.heads = [Head(self.head_size, dropout = self.dropout) for _ in range(self.num_heads)]\n",
    "        self.projection = tf.keras.layers.Dense(input_shape[-1])\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x, training =False):\n",
    "        out = tf.concat([h(x, training = training) for h in self.heads], axis=-1)\n",
    "        out = self.projection(out)\n",
    "###\n",
    "        return self.dropout(out, training=training)\n",
    "###\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    '''A simple linear layer followed by a non-linearity'''\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4 * n_embed), # (n_embed, 4 * n_embed)\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Dense(n_embed), # (4 * n_embed, n_embed)\n",
    "###\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "###\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training):\n",
    "        return self.net(x, training = training)\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    \"\"\"Transformer blocks : communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        # n_embed : embedding dimension, n_head : the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dropout) # Communication\n",
    "        self.ffwd = FeedForward(n_embed, dropout) # Computation of individual tokens\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "        # Residual Connections to preserve information, and improve gradient flow\n",
    "        x = x + self.sa(self.ln1(x), training = training)\n",
    "        x = x + self.ffwd(self.ln2(x), training = training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        '''Initializing embedding layer, which maps integer indices to\n",
    "        dense vectors of vocab size'''\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = tf.keras.layers.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = tf.keras.layers.Embedding(block_size, n_embed)\n",
    "###\n",
    "        self.blocks = [ Block(n_embed, n_head, dropout) for _ in range(n_layer)]\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "###\n",
    "        self.lm_head = tf.keras.layers.Dense(units=vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None, training = False):\n",
    "        '''Method for loss calculation, based on idx (input token indices) and\n",
    "        target (target token indices)\n",
    "        B : Batch size\n",
    "        T : Time = block size = sequence length\n",
    "        C : Channel = vocab size = number of classes\n",
    "        '''\n",
    "        B,T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C) Replacing indices with embeddings\n",
    "        pos_emb = self.position_embedding_table(tf.range(T, dtype=tf.int32)) # (T,C)\n",
    "        x = token_emb + pos_emb # (B, T, C) Containing both token embedding and position\n",
    "\n",
    "        #Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "          x = block(x, training = training)\n",
    "\n",
    "        x = self.ln_f(x) # Apply normalization\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None: # If target is not provided\n",
    "            loss = None\n",
    "        else:               # If target is provided, reshape the tensor so that it's compatible with categorical cross entropy\n",
    "            B, T, C = tf.shape(logits) # Get the shape of logits\n",
    "            logits = tf.reshape(logits, (B * T, C)) # Flatten logits for comparison\n",
    "            targets = tf.reshape(targets, (B * T,)) # Flatten targets\n",
    "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        '''\n",
    "        Text generating method\n",
    "        '''\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens to avoid going out of scope\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step (i.e. history is not being used)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = tf.nn.softmax(logits, axis=-1)  # (B, C)\n",
    "            # One sample prediction from the distribution\n",
    "            idx_next = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int64) # (B, 1)\n",
    "\n",
    "            # idx_next = tf.random.categorical(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, tf.cast(idx_next, tf.int32)], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model_scaled = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\t\t train loss 4.6753 | val loss 4.6567\n",
      "Step 200\t train loss 2.5079 | val loss 2.5134 | time 1 min 18 seconds\n",
      "Step 400\t train loss 2.3489 | val loss 2.3582 | time 1 min 17 seconds\n",
      "Step 600\t train loss 2.2080 | val loss 2.2454 | time 1 min 17 seconds\n",
      "Step 800\t train loss 2.1310 | val loss 2.1803 | time 1 min 14 seconds\n",
      "Step 1000\t train loss 2.0606 | val loss 2.1222 | time 1 min 14 seconds\n",
      "Step 1200\t train loss 2.0137 | val loss 2.0915 | time 1 min 13 seconds\n",
      "Step 1400\t train loss 1.9821 | val loss 2.0676 | time 1 min 13 seconds\n",
      "Step 1600\t train loss 1.9366 | val loss 2.0297 | time 1 min 10 seconds\n",
      "Step 1800\t train loss 1.9045 | val loss 2.0060 | time 1 min 11 seconds\n",
      "Step 2000\t train loss 1.8825 | val loss 1.9942 | time 1 min 9 seconds\n",
      "Step 2200\t train loss 1.8597 | val loss 1.9729 | time 1 min 7 seconds\n",
      "Step 2400\t train loss 1.8489 | val loss 1.9775 | time 1 min 10 seconds\n",
      "Step 2600\t train loss 1.8384 | val loss 1.9694 | time 1 min 8 seconds\n",
      "Step 2800\t train loss 1.8234 | val loss 1.9574 | time 1 min 5 seconds\n",
      "Step 3000\t train loss 1.8032 | val loss 1.9544 | time 1 min 7 seconds\n",
      "Step 3200\t train loss 1.7920 | val loss 1.9282 | time 1 min 9 seconds\n",
      "Step 3400\t train loss 1.7836 | val loss 1.9193 | time 1 min 7 seconds\n",
      "Step 3600\t train loss 1.7718 | val loss 1.9014 | time 1 min 5 seconds\n",
      "Step 3800\t train loss 1.7547 | val loss 1.9104 | time 1 min 6 seconds\n",
      "Step 4000\t train loss 1.7416 | val loss 1.8970 | time 1 min 6 seconds\n",
      "Final Loss: 1.7354952096939087\n",
      "\n",
      "======================= Generated Sequence =======================\n",
      "\n",
      "First of th!\n",
      "That you, if done oftlew\n",
      "And engrouth, thethink iblo; that seaght, out!\n",
      "\n",
      "CFRIV:\n",
      "How How Bolowo\n",
      "Namer in' belconverver grack Herew come,\n",
      "well, Rich goods, abron, nor shall shall luce for thigh fair,\n",
      "But new the proscourt\n",
      "I, shalf in\n",
      "What I shall, thin, by should more come.\n",
      "\n",
      "CLISAR:\n",
      "My doithing them:\n",
      "Gor morwind telvess', the child,\n",
      "Dell Eard ot blood man sagain, I dear in from most you she intid,\n",
      "Cour'd and ternent make ampon I crouck.\n",
      "\n",
      "ESBRULY:\n",
      "Say, Rowelon?\n",
      "\n",
      "WARWICK:\n",
      "There? med be'\n"
     ]
    }
   ],
   "source": [
    "model_train(model_scaled, 'Scaled')\n",
    "model_generate(model_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Val loss</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>2.4711</td>\n",
       "      <td>2.4878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>\\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-attention</td>\n",
       "      <td>2.3557</td>\n",
       "      <td>2.3745</td>\n",
       "      <td>8.0</td>\n",
       "      <td>\\nGrth berded ourdar to, hey, herirod;, therd yod wino!\\nWARNG:\\nBes,\\nI fad wis the: isth gears cituy f ou boerilercors yon,\\nThord thinticer ofo wieche I INorurdea randexalan.\\nNou say\\nAnel fou byo cy. ARCEO:\\nLou Ene toe, paer steran forve Pou\\nies th aried inlon uan thelem pe't\\nsical akitaly, mave,-juegh.\\n\\nAn'TO:\\nTAno\\nWhir\\nLaveit'd; thind 'sourd.\\n\\nBELLO ave me he?\\nINou y thels ld\\nWhexy ato dove wimarenas by at\\nBur. KARTHanit maved,\\nABRUK:\\n\\nBy; osd ansch pert torow'd comicewhe ke!\\nFak, gythesard tey q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muti-headed attention</td>\n",
       "      <td>2.1920</td>\n",
       "      <td>2.2168</td>\n",
       "      <td>11.0</td>\n",
       "      <td>\\nCOROProm:\\nI geast mest bl fath my hist ss arysupy\\nAn?\\nPEAERCIDA:\\nIN a thembutt,\\nPay avecse, aswe k's bas ters no grark, klive's.\\nKE WIG irt;\\nWer\\nFot lasupave penot you pe bibe! of thy ty me-he; I o be han, arizede yor yougs timpid, band dend the 'd weit my stragn.\\n\\nFas ma this showio himm by cingee o, for cen's!\\nHARDILO:\\nI dys thered.\\nAnd coul the 'swse? ay, anent thrid im's\\nD INIUS:\\nAnd, aglent, Le OF eve o wamledse noumse I yot,'lse wit I ter ball hy leengin bown.\\n\\nCithine\\nHen mbe lore bo whew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feed forward</td>\n",
       "      <td>2.1342</td>\n",
       "      <td>2.1828</td>\n",
       "      <td>31.0</td>\n",
       "      <td>\\nTo be now as lodn.\\n\\nBENCIHUS:\\nThe of the hail'l?\\nYored ouark taine sins. if to wruse:\\nnot bickus the a onay pern thape silone therem.\\nbom the aence,\\nThath ome ounceing magy thisen.\\nNard omlat; bas plailne? of oskes!\\nYourm.r ther olit oun to sepucesfirse lied. baissh huny; goops oupeen, Tiver in nor thee ingthech thouech, I farter er'p\\nlath thee hake: a thath,\\nTo you the cise merting astert tilstlevoe: maight, nonmmlys\\nI ound, eoccuz poiedaung\\nFoorl ither's a and townt,\\nOw ece.srfeer have ageand,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimized</td>\n",
       "      <td>1.8519</td>\n",
       "      <td>1.9671</td>\n",
       "      <td>38.0</td>\n",
       "      <td>\\nThe ridpers rrace shall I ackie!\\nIfty to gate to pese have tread it:\\nA matelliegly lery tcounte uppoter.\\n\\nVOLINIUS:\\nE:\\nAze hid of a somers deep'lle have for unwordy.\\nAlack them\\nAstin to o as me! way I\\nThat 'cles thou with them lears\\nbut wate kin harnicbiod, thy laud in we rifh\\nReced whith say love to manite mode beate.\\n\\nBELID:\\nYou my, cast thin the pold, tell dephars. Mire a cuing onclesmlance your,\\nOn you hisgt ate to burnuege, trots gontedn; on tesw?\\nDo she amper you' lather of hasir Venrowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Scaled</td>\n",
       "      <td>1.7416</td>\n",
       "      <td>1.8970</td>\n",
       "      <td>24.0</td>\n",
       "      <td>\\nFirst of th!\\nThat you, if done oftlew\\nAnd engrouth, thethink iblo; that seaght, out!\\n\\nCFRIV:\\nHow How Bolowo\\nNamer in' belconverver grack Herew come,\\nwell, Rich goods, abron, nor shall shall luce for thigh fair,\\nBut new the proscourt\\nI, shalf in\\nWhat I shall, thin, by should more come.\\n\\nCLISAR:\\nMy doithing them:\\nGor morwind telvess', the child,\\nDell Eard ot blood man sagain, I dear in from most you she intid,\\nCour'd and ternent make ampon I crouck.\\n\\nESBRULY:\\nSay, Rowelon?\\n\\nWARWICK:\\nThere? med be'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train loss  Val loss  Time (min)  \\\n",
       "0                  Basic      2.4711    2.4878         4.0   \n",
       "1         Self-attention      2.3557    2.3745         8.0   \n",
       "2  Muti-headed attention      2.1920    2.2168        11.0   \n",
       "3           Feed forward      2.1342    2.1828        31.0   \n",
       "4              Optimized      1.8519    1.9671        38.0   \n",
       "5                 Scaled      1.7416    1.8970        24.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Text  \n",
       "0            \\nD g'sou\\nG mese;\\nWe RWAmey stan fogas Gis w,\\nTand,parit amae ghe!nd acoracore.\\nS:\\nAY s hend y VI ll t t ace ben;\\nWhee t thapoush, tes flyogen qhe\\nThare is-fe wau, f way s, rcoooua d ik; as ist theaxe onthiteereatlalit; tey, d t hyXjund thice te oak nga as irn blon, m n n\\n\\n\\n\\nWhe n.\\n\\nRDNCI thoungot y s, hean y ILTI wiach nd t un lled d abethal t the tie dacovey th sel'ty tind gu figbndstarthedire and withes ousthad besthind w st s wrmony, utyoor gyXRCEROL gacind t lathey f ws he wthbed mopowesouth   \n",
       "1  \\nGrth berded ourdar to, hey, herirod;, therd yod wino!\\nWARNG:\\nBes,\\nI fad wis the: isth gears cituy f ou boerilercors yon,\\nThord thinticer ofo wieche I INorurdea randexalan.\\nNou say\\nAnel fou byo cy. ARCEO:\\nLou Ene toe, paer steran forve Pou\\nies th aried inlon uan thelem pe't\\nsical akitaly, mave,-juegh.\\n\\nAn'TO:\\nTAno\\nWhir\\nLaveit'd; thind 'sourd.\\n\\nBELLO ave me he?\\nINou y thels ld\\nWhexy ato dove wimarenas by at\\nBur. KARTHanit maved,\\nABRUK:\\n\\nBy; osd ansch pert torow'd comicewhe ke!\\nFak, gythesard tey q  \n",
       "2       \\nCOROProm:\\nI geast mest bl fath my hist ss arysupy\\nAn?\\nPEAERCIDA:\\nIN a thembutt,\\nPay avecse, aswe k's bas ters no grark, klive's.\\nKE WIG irt;\\nWer\\nFot lasupave penot you pe bibe! of thy ty me-he; I o be han, arizede yor yougs timpid, band dend the 'd weit my stragn.\\n\\nFas ma this showio himm by cingee o, for cen's!\\nHARDILO:\\nI dys thered.\\nAnd coul the 'swse? ay, anent thrid im's\\nD INIUS:\\nAnd, aglent, Le OF eve o wamledse noumse I yot,'lse wit I ter ball hy leengin bown.\\n\\nCithine\\nHen mbe lore bo whew  \n",
       "3           \\nTo be now as lodn.\\n\\nBENCIHUS:\\nThe of the hail'l?\\nYored ouark taine sins. if to wruse:\\nnot bickus the a onay pern thape silone therem.\\nbom the aence,\\nThath ome ounceing magy thisen.\\nNard omlat; bas plailne? of oskes!\\nYourm.r ther olit oun to sepucesfirse lied. baissh huny; goops oupeen, Tiver in nor thee ingthech thouech, I farter er'p\\nlath thee hake: a thath,\\nTo you the cise merting astert tilstlevoe: maight, nonmmlys\\nI ound, eoccuz poiedaung\\nFoorl ither's a and townt,\\nOw ece.srfeer have ageand,  \n",
       "4         \\nThe ridpers rrace shall I ackie!\\nIfty to gate to pese have tread it:\\nA matelliegly lery tcounte uppoter.\\n\\nVOLINIUS:\\nE:\\nAze hid of a somers deep'lle have for unwordy.\\nAlack them\\nAstin to o as me! way I\\nThat 'cles thou with them lears\\nbut wate kin harnicbiod, thy laud in we rifh\\nReced whith say love to manite mode beate.\\n\\nBELID:\\nYou my, cast thin the pold, tell dephars. Mire a cuing onclesmlance your,\\nOn you hisgt ate to burnuege, trots gontedn; on tesw?\\nDo she amper you' lather of hasir Venrowing  \n",
       "5   \\nFirst of th!\\nThat you, if done oftlew\\nAnd engrouth, thethink iblo; that seaght, out!\\n\\nCFRIV:\\nHow How Bolowo\\nNamer in' belconverver grack Herew come,\\nwell, Rich goods, abron, nor shall shall luce for thigh fair,\\nBut new the proscourt\\nI, shalf in\\nWhat I shall, thin, by should more come.\\n\\nCLISAR:\\nMy doithing them:\\nGor morwind telvess', the child,\\nDell Eard ot blood man sagain, I dear in from most you she intid,\\nCour'd and ternent make ampon I crouck.\\n\\nESBRULY:\\nSay, Rowelon?\\n\\nWARWICK:\\nThere? med be'  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
